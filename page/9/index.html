<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Movle</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="https://img-blog.csdnimg.cn/20200609161448519.jpg" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="https://img-blog.csdnimg.cn/2020060916514052.png" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Movle</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['种一棵树，最好的时机是十年前，其次是现在', '人必有痴，而后有成', '今天，我没有浑浑噩噩的度过'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-Spark实战-用Scala编写WordCount程序"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/12/Spark%E5%AE%9E%E6%88%98-%E7%94%A8Scala%E7%BC%96%E5%86%99WordCount%E7%A8%8B%E5%BA%8F/"
    >Spark实战-用Scala编写WordCount程序</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/12/Spark%E5%AE%9E%E6%88%98-%E7%94%A8Scala%E7%BC%96%E5%86%99WordCount%E7%A8%8B%E5%BA%8F/" class="article-date">
  <time datetime="2020-03-12T05:00:00.000Z" itemprop="datePublished">2020-03-12</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-添加pom依赖："><a href="#一-添加pom依赖：" class="headerlink" title="一.添加pom依赖："></a>一.添加pom依赖：</h3><p>pom.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>compile-scala<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>add-source<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>test-compile-scala<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>test-compile<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>add-source<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="二-编写代码："><a href="#二-编写代码：" class="headerlink" title="二.编写代码："></a>二.编写代码：</h3><h4 id="1-本地模式："><a href="#1-本地模式：" class="headerlink" title="1.本地模式："></a>1.本地模式：</h4><p>WordCount.scala</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> WordCoutScala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"></span><br><span class="line">object WordCount &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//定义主方法</span></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkConf对象</span></span><br><span class="line">    <span class="comment">//如果Master是local，表示运行在本地模式上，即可以在开发工具中直接运行</span></span><br><span class="line">    <span class="comment">//如果要提交到集群中运行，不需要设置Master</span></span><br><span class="line">    <span class="comment">//集群模式</span></span><br><span class="line">    <span class="comment">//val conf = new SparkConf().setAppName(&quot;My Scala Word Count&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//本地模式</span></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;My Scala Word Count&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkContext对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line"></span><br><span class="line">        val result = sc.textFile(<span class="string">&quot;hdfs://192.168.1.120:9000/TestFile/test_WordCount.txt&quot;</span>)</span><br><span class="line">                        .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">                        .map((_,<span class="number">1</span>))</span><br><span class="line">                        .reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">         result.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//集群模式</span></span><br><span class="line"><span class="comment">//    val result = sc.textFile(args(0))</span></span><br><span class="line"><span class="comment">//      .flatMap(_.split(&quot; &quot;))</span></span><br><span class="line"><span class="comment">//      .map((_,1))</span></span><br><span class="line"><span class="comment">//      .reduceByKey(_+_)</span></span><br><span class="line"><span class="comment">//      .saveAsTextFile(args(1))</span></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWU3Nzk3MjRkYzUwZjA0MmIucG5n?x-oss-process=image/format,png" alt="结果"></p>
<h4 id="2-集群模式："><a href="#2-集群模式：" class="headerlink" title="2.集群模式："></a>2.集群模式：</h4><h5 id="1-编写WordCount-scala"><a href="#1-编写WordCount-scala" class="headerlink" title="(1)编写WordCount.scala"></a>(1)编写WordCount.scala</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"></span><br><span class="line">object WordCount &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//定义主方法</span></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建SparkConf对象</span></span><br><span class="line">    <span class="comment">//如果Master是local，表示运行在本地模式上，即可以在开发工具中直接运行</span></span><br><span class="line">    <span class="comment">//如果要提交到集群中运行，不需要设置Master</span></span><br><span class="line">    <span class="comment">//集群模式</span></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;My Scala Word Count&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//本地模式</span></span><br><span class="line">    <span class="comment">//val conf = new SparkConf().setAppName(&quot;My Scala Word Count&quot;).setMaster(&quot;local&quot;)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建SparkContext对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line">    </span><br><span class="line"><span class="comment">//    val result = sc.textFile(&quot;hdfs://192.168.1.120:9000/TestFile/test_WordCount.txt&quot;)  </span></span><br><span class="line"><span class="comment">//                    .flatMap(_.split(&quot; &quot;))</span></span><br><span class="line"><span class="comment">//                    .map((_,1))</span></span><br><span class="line"><span class="comment">//                    .reduceByKey(_+_)</span></span><br><span class="line"><span class="comment">//                    </span></span><br><span class="line"><span class="comment">//     result.foreach(println)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//集群模式</span></span><br><span class="line">    val result = sc.textFile(args(<span class="number">0</span>))  </span><br><span class="line">                .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">                .map((_,<span class="number">1</span>)) </span><br><span class="line">                .reduceByKey(_+_)</span><br><span class="line">                .saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-打包"><a href="#2-打包" class="headerlink" title="(2)打包"></a>(2)打包</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWZlYzg4YmQxMjY2ZjZjMTMucG5n?x-oss-process=image/format,png" alt="打包"></p>
<h5 id="3-上传到Spark节点："><a href="#3-上传到Spark节点：" class="headerlink" title="(3)上传到Spark节点："></a>(3)上传到Spark节点：</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTIzMDY1YzNiMzc2MjI5MzEucG5n?x-oss-process=image/format,png" alt="上传"></p>
<h5 id="4-运行："><a href="#4-运行：" class="headerlink" title="(4)运行："></a>(4)运行：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://hadoop:7077 --class WordCoutScala.WordCount /opt/TestFile/ScalaProject-1.0-SNAPSHOT.jar hdfs://hadoop:9000/TestFile/test_WordCount.txt hdfs://hadoop:9000/output/1209/demo1</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTYxMTQxNWZiMjdmZTBkYzAucG5n?x-oss-process=image/format,png" alt="运行"></p>
<h5 id="5-结果："><a href="#5-结果：" class="headerlink" title="(5)结果："></a>(5)结果：</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWViNGU4YTY5NWVjOTc4ZjcucG5n?x-oss-process=image/format,png" alt="image.png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark%E5%AE%9E%E6%88%98/" rel="tag">Spark实战</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark实战-在Spark Shell中开发一个wordcount程序"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/12/Spark%E5%AE%9E%E6%88%98-%E5%9C%A8Spark%20Shell%E4%B8%AD%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AAwordcount%E7%A8%8B%E5%BA%8F/"
    >Spark实战-在Spark Shell中开发一个wordcount程序</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/12/Spark%E5%AE%9E%E6%88%98-%E5%9C%A8Spark%20Shell%E4%B8%AD%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AAwordcount%E7%A8%8B%E5%BA%8F/" class="article-date">
  <time datetime="2020-03-12T04:00:00.000Z" itemprop="datePublished">2020-03-12</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-读取一个本地文件，将结果打印到屏幕上。"><a href="#1-读取一个本地文件，将结果打印到屏幕上。" class="headerlink" title="1.读取一个本地文件，将结果打印到屏幕上。"></a>1.读取一个本地文件，将结果打印到屏幕上。</h4><p>注意：示例必须只有一个worker 且本地文件与worker在同一台服务器上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> sc.textFile(<span class="string">&quot;/opt/TestFile/test_WordCount.txt&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,1)).reduceByKey(_+_).collect</span></span><br><span class="line">```			</span><br><span class="line">![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWUwZmVmMTQwZDMzOTYzZjQucG5n?x-oss-process=image/format,png)		</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">### 2.读取一个hdfs文件，进行WordCount操作，并将结果写回hdfs</span></span></span><br><span class="line">```shell</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> sc.textFile(<span class="string">&quot;hdfs://hadoop:9000/TestFile/test_WordCount.txt&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,1)).reduceByKey(_+_).saveAsTextFile(<span class="string">&quot;hdfs://hadoop:9000/output/1208&quot;</span>)</span></span><br><span class="line">			</span><br><span class="line">[root@hadoop sbin]# hadoop dfs -ls /output/1208</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWZjZTJkYjY3YzMzOTUzMTQucG5n?x-oss-process=image/format,png" alt="输入命令"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTJlM2EyYmU2MWY3MTRmZjcucG5n?x-oss-process=image/format,png" alt="结果"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWY3MzUzYmQxMDYxNTgyMmIucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-单步运行WordCount-—–-gt-RDD"><a href="#3-单步运行WordCount-—–-gt-RDD" class="headerlink" title="3.单步运行WordCount  —–&gt;  RDD"></a>3.单步运行WordCount  —–&gt;  RDD</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd1 = sc.textFile(<span class="string">&quot;/opt/TestFile/test_WordCount.txt&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd1.collect</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd2 = rdd1.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd2.collect</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd3 = rdd2.map((_,1))</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd3.collect</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd4 = rdd3.reduceByKey(_+_)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd4.collect</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWU5NGM3ZDYxMTUwNWQyMjgucG5n?x-oss-process=image/format,png"></p>
<h5 id="1-RDD说明：RDD-弹性分布式数据集"><a href="#1-RDD说明：RDD-弹性分布式数据集" class="headerlink" title="(1)RDD说明：RDD 弹性分布式数据集"></a>(1)RDD说明：RDD 弹性分布式数据集</h5><h5 id="2-特性："><a href="#2-特性：" class="headerlink" title="(2)特性："></a>(2)特性：</h5><h6 id="a-依赖关系-rdd2依赖于rdd1"><a href="#a-依赖关系-rdd2依赖于rdd1" class="headerlink" title="(a)依赖关系:rdd2依赖于rdd1"></a>(a)依赖关系:rdd2依赖于rdd1</h6><h6 id="b-算子"><a href="#b-算子" class="headerlink" title="(b)算子"></a>(b)算子</h6><ul>
<li>Transformation    延时计算: flatMap,map,reduceByKey</li>
<li>Action  触发计算:collect</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark%E5%AE%9E%E6%88%98/" rel="tag">Spark实战</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark SQL：性能优化"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/11/Spark%20SQL%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"
    >Spark SQL:性能优化</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/11/Spark%20SQL%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" class="article-date">
  <time datetime="2020-03-11T04:00:00.000Z" itemprop="datePublished">2020-03-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-在内存中缓存数据"><a href="#1-在内存中缓存数据" class="headerlink" title="1.在内存中缓存数据"></a>1.在内存中缓存数据</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;性能调优主要是将数据放入内存中操作。通过spark.cacheTable(“tableName”)或者dataFrame.cache()。使用spark.uncacheTable(“tableName”)来从内存中去除table。</p>
<p><strong>Demo案例：</strong></p>
<h5 id="1-从Oracle数据库中读取数据，生成DataFrame"><a href="#1-从Oracle数据库中读取数据，生成DataFrame" class="headerlink" title="(1)从Oracle数据库中读取数据，生成DataFrame"></a>(1)从Oracle数据库中读取数据，生成DataFrame</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val mysqlDF = spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://192.168.109.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;dbtable&quot;</span>,<span class="string">&quot;company.emp&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;000000&quot;</span>).load</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val mysqlDF = spark.read.format(<span class="string">&quot;jdbc&quot;</span>).option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://192.168.109.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;</span>).option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;000000&quot;</span>).option(<span class="string">&quot;dbtable&quot;</span>,<span class="string">&quot;emp&quot;</span>).option(<span class="string">&quot;driver&quot;</span>,<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>).load</span><br></pre></td></tr></table></figure>

<h5 id="2-将DataFrame注册成表："><a href="#2-将DataFrame注册成表：" class="headerlink" title="(2)将DataFrame注册成表："></a>(2)将DataFrame注册成表：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlDF.registerTempTable(<span class="string">&quot;emp&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>注意：必须注册成一张表，才可以缓存</p>
<h5 id="3-执行查询，并通过Web-Console监控执行的时间"><a href="#3-执行查询，并通过Web-Console监控执行的时间" class="headerlink" title="(3)执行查询，并通过Web Console监控执行的时间"></a>(3)执行查询，并通过Web Console监控执行的时间</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;select * from emp&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRjODJkYzJkOTE0ZDA0YmYucG5n?x-oss-process=image/format,png"></p>
<h5 id="4-将表进行缓存，并查询两次，并通过Web-Console监控执行的时间"><a href="#4-将表进行缓存，并查询两次，并通过Web-Console监控执行的时间" class="headerlink" title="(4)将表进行缓存，并查询两次，并通过Web Console监控执行的时间"></a>(4)将表进行缓存，并查询两次，并通过Web Console监控执行的时间</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sqlContext.cacheTable(&quot;emp&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from emp&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWE0ZjkxOGNjMDU5NzllOWMucG5n?x-oss-process=image/format,png"></p>
<h5 id="5-清空缓存："><a href="#5-清空缓存：" class="headerlink" title="(5)清空缓存："></a>(5)清空缓存：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sqlContext.cacheTable(<span class="string">&quot;emp&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sqlContext.clearCache</span><br></pre></td></tr></table></figure>
<h4 id="2-性能优化相关参数"><a href="#2-性能优化相关参数" class="headerlink" title="2.性能优化相关参数"></a>2.性能优化相关参数</h4><h5 id="1-将数据缓存到内存中的相关优化参数"><a href="#1-将数据缓存到内存中的相关优化参数" class="headerlink" title="(1)将数据缓存到内存中的相关优化参数"></a>(1)将数据缓存到内存中的相关优化参数</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.inMemoryColumnarStorage.compressed</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认为 <span class="literal">true</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>Spark <span class="keyword">SQL</span> 将会基于统计信息自动地为每一列选择一种压缩编码方式。</span><br><span class="line"></span><br><span class="line">spark.sql.inMemoryColumnarStorage.batchSize</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认值：<span class="number">10000</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（<span class="keyword">Out</span> <span class="keyword">Of</span> Memory）的风险。</span><br></pre></td></tr></table></figure>
<h5 id="2-其他性能相关的配置选项-不过不推荐手动修改，可能在后续版本自动的自适应修改"><a href="#2-其他性能相关的配置选项-不过不推荐手动修改，可能在后续版本自动的自适应修改" class="headerlink" title="(2)其他性能相关的配置选项(不过不推荐手动修改，可能在后续版本自动的自适应修改)"></a>(2)其他性能相关的配置选项(不过不推荐手动修改，可能在后续版本自动的自适应修改)</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.files.maxPartitionBytes</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认值：<span class="number">128</span> MB</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>读取文件时单个分区可容纳的最大字节数</span><br><span class="line"></span><br><span class="line">spark.sql.files.openCostInBytes</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认值：<span class="number">4</span>M</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)。</span><br><span class="line"></span><br><span class="line">spark.sql.autoBroadcastJoinThreshold</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认值：<span class="number">10</span>M</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>用于配置一个表在执行 <span class="keyword">join</span> 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 <span class="number">-1</span> 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE <span class="keyword">TABLE</span> <span class="operator">&lt;</span>tableName<span class="operator">&gt;</span> COMPUTE STATISTICS noscan 命令的 Hive Metastore 表。</span><br><span class="line"></span><br><span class="line">spark.sql.shuffle.partitions</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>默认值：<span class="number">200</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>用于配置 <span class="keyword">join</span> 或聚合操作混洗(shuffle)数据时使用的分区数。</span><br></pre></td></tr></table></figure>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL/" rel="tag">Spark SQL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark SQL：使用数据源"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/11/Spark%20SQL%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E6%BA%90/"
    >Spark SQL:使用数据源</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/11/Spark%20SQL%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E6%BA%90/" class="article-date">
  <time datetime="2020-03-11T03:00:00.000Z" itemprop="datePublished">2020-03-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>&nbsp;&nbsp;&nbsp;&nbsp;在Spark SQL中，可以使用各种各样的数据源进行操作。Spark SQL 用于处理结构化的数据</p>
<h3 id="一-通用的Load-Save函数-load函数式加载数据，save函数式存储数据"><a href="#一-通用的Load-Save函数-load函数式加载数据，save函数式存储数据" class="headerlink" title="一.通用的Load/Save函数(load函数式加载数据，save函数式存储数据)"></a>一.通用的Load/Save函数(load函数式加载数据，save函数式存储数据)</h3><p>&nbsp;&nbsp;&nbsp;注意：使用load或者save函数时，默认的数据源都是 Parquet文件。列式存储文件</p>
<h4 id="1-通用的Load-Save函数"><a href="#1-通用的Load-Save函数" class="headerlink" title="1.通用的Load/Save函数"></a>1.通用的Load/Save函数</h4><h5 id="1-读取Parquet文件"><a href="#1-读取Parquet文件" class="headerlink" title="(1)读取Parquet文件"></a>(1)读取Parquet文件</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(<span class="string">&quot;/root/resources/users.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="2-查询Schema和数据"><a href="#2-查询Schema和数据" class="headerlink" title="(2)查询Schema和数据"></a>(2)查询Schema和数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; usersDF.printSchema</span><br><span class="line"></span><br><span class="line">scala&gt; usersDF.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTM5NThiMGYzMmQ1MTI3NjMucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-查询用户的name和喜爱颜色，并保存"><a href="#3-查询用户的name和喜爱颜色，并保存" class="headerlink" title="(3)查询用户的name和喜爱颜色，并保存"></a>(3)查询用户的name和喜爱颜色，并保存</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select($<span class="string">&quot;name&quot;</span>,$<span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;/root/result/parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="4-验证结果"><a href="#4-验证结果" class="headerlink" title="(4)验证结果"></a>(4)验证结果</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;val testResult = spark.read.load(<span class="string">&quot;/root/result/parquet/part-00000-8ffaac2e-aa81-4e63-89aa-15a8e4948a37.snappy.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala &gt;testResult.printSchema</span><br><span class="line"></span><br><span class="line">scala &gt;testResult.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTNmYmU3ODU0OTg4NGU4ZTQucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-显式指定文件格式：加载json格式"><a href="#2-显式指定文件格式：加载json格式" class="headerlink" title="2.显式指定文件格式：加载json格式"></a>2.显式指定文件格式：加载json格式</h4><h5 id="1-直接加载："><a href="#1-直接加载：" class="headerlink" title="(1)直接加载："></a>(1)直接加载：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(<span class="string">&quot;/root/resources/people.json&quot;</span>)  <span class="comment">//会出错</span></span><br></pre></td></tr></table></figure>
<p>上面这个会出错</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/root/resources/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-存储模式（Save-Modes）"><a href="#3-存储模式（Save-Modes）" class="headerlink" title="3.存储模式（Save Modes）"></a>3.存储模式（Save Modes）</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQ2YTAyOGMwMjVkN2U0N2YucG5n?x-oss-process=image/format,png"><br>Demo：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select($<span class="string">&quot;name&quot;</span>).write.save(<span class="string">&quot;/root/result/parquet1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>上面出错：因为/root/result/parquet1已经存在</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select($<span class="string">&quot;name&quot;</span>).write.mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/root/result/parquet1&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="4-将结果保存为表"><a href="#4-将结果保存为表" class="headerlink" title="4.将结果保存为表"></a>4.将结果保存为表</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usersDF.select($<span class="string">&quot;name&quot;</span>).write.saveAsTable(<span class="string">&quot;table1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>也可以进行分区、分桶等操作：partitionBy、bucketBy</p>
<h3 id="二-Parquet文件-列式存储文件，是Spark-SQL默认的数据源"><a href="#二-Parquet文件-列式存储文件，是Spark-SQL默认的数据源" class="headerlink" title="二.Parquet文件(列式存储文件，是Spark SQL默认的数据源)"></a>二.Parquet文件(列式存储文件，是Spark SQL默认的数据源)</h3><h4 id="1-什么是parquet文件？"><a href="#1-什么是parquet文件？" class="headerlink" title="1.什么是parquet文件？"></a>1.什么是parquet文件？</h4><h5 id="1-Parquet是列式存储格式的一种文件类型，列式存储有以下的核心："><a href="#1-Parquet是列式存储格式的一种文件类型，列式存储有以下的核心：" class="headerlink" title="(1)Parquet是列式存储格式的一种文件类型，列式存储有以下的核心："></a>(1)Parquet是列式存储格式的一种文件类型，列式存储有以下的核心：</h5><ul>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
<li>Parquet格式是Spark SQL的默认数据源，可通过spark.sql.sources.default配置</li>
</ul>
<h5 id="2-Parquet是一个列格式而且用于多个数据处理系统中。"><a href="#2-Parquet是一个列格式而且用于多个数据处理系统中。" class="headerlink" title="(2)Parquet是一个列格式而且用于多个数据处理系统中。"></a>(2)Parquet是一个列格式而且用于多个数据处理系统中。</h5><p>Spark SQL提供支持对于Parquet文件的读写，也就是自动保存原始数据的schema。当写Parquet文件时，所有的列被自动转化为nullable，因为兼容性的缘故。</p>
<h4 id="2-把其他文件，转换成Parquet文件"><a href="#2-把其他文件，转换成Parquet文件" class="headerlink" title="2.把其他文件，转换成Parquet文件()"></a>2.把其他文件，转换成Parquet文件()</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;读入json格式的数据，将其转换成parquet格式，并创建相应的表来使用SQL进行查询。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;val empDF = spark.read.json(<span class="string">&quot;/opt/module/datas/TestFile/emp.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala &gt;empDF.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LThjNDdhNmU2ZTU5MTk3ZGIucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;empDF.write.mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/opt/module/datas/TestFile/myresult/parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//save和parquet都可以写入，是一样的</span></span><br><span class="line">scala &gt;empDF.write.mode(<span class="string">&quot;overwrite&quot;</span>).parquet(<span class="string">&quot;/opt/module/datas/TestFile/myresult/parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala &gt;val emp1=spark.read.parquet(<span class="string">&quot;/opt/module/datas/TestFile/myresult/parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala &gt;emp1.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWZiZDgxNGFmNmQwMTk2NjIucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;emp1.createOrReplaceTempView(<span class="string">&quot;emptable&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala &gt;spark.sql(<span class="string">&quot;select * from emptable where deptno =10 and sal &gt;1500&quot;</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTg4ZTAwMzcxMDAxYjdkOWUucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-支持Schema的合并："><a href="#3-支持Schema的合并：" class="headerlink" title="3.支持Schema的合并："></a>3.支持Schema的合并：</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Parquet支持Schema evolution(Schema演变，即：合并)。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。<br>Demo:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val df1= sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i,i*<span class="number">2</span>)).toDF(<span class="string">&quot;single&quot;</span>,<span class="string">&quot;double&quot;</span>)</span><br><span class="line"></span><br><span class="line">df1.show</span><br><span class="line"></span><br><span class="line">sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).collect</span><br><span class="line"></span><br><span class="line">df1.write.parquet(<span class="string">&quot;/opt/module/datas/TestFile/test_table/key=1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWViODVkOTA5M2M1MTg0MzgucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i=&gt;(i,i*<span class="number">3</span>)).toDF(<span class="string">&quot;single&quot;</span>,<span class="string">&quot;triple&quot;</span>)</span><br><span class="line"></span><br><span class="line">df2.show</span><br><span class="line"></span><br><span class="line">df2.write.parquet(<span class="string">&quot;/opt/module/datas/TestFile/test_table/key=2&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWM2NTFhMDE3M2I1YzJlNTIucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df3 = spark.read.parquet(<span class="string">&quot;/opt/module/datas/TestFile/test_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">df3.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTFjMGQ5ODIyM2MwMDNjZDIucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df3 = spark.read.option(<span class="string">&quot;mergeSchema&quot;</span>,<span class="keyword">true</span>).parquet(<span class="string">&quot;/opt/module/datas/TestFile/test_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">df3.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE0OGRiOTMzMTVhN2ZhZjcucG5n?x-oss-process=image/format,png"></p>
<h4 id="4-JSON-文件"><a href="#4-JSON-文件" class="headerlink" title="4.JSON 文件"></a>4.JSON 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL能自动解析JSON数据集的Schema，读取JSON数据集为DataFrame格式。读取JSON数据集方法为SQLContext.read().json()。该方法将String格式的RDD或JSON文件转换为DataFrame。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是，这里的JSON文件不是常规的JSON格式。JSON文件每一行必须包含一个独立的、自满足有效的JSON对象。如果用多行描述一个JSON对象，会导致读取出错。读取JSON数据集示例如下：</p>
<h5 id="1-Demo1：使用Spark自带的示例文件-–-gt-people-json-文件"><a href="#1-Demo1：使用Spark自带的示例文件-–-gt-people-json-文件" class="headerlink" title="(1)Demo1：使用Spark自带的示例文件 –&gt; people.json 文件"></a>(1)Demo1：使用Spark自带的示例文件 –&gt; people.json 文件</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义路径：</span></span><br><span class="line">val path =<span class="string">&quot;/opt/module/datas/TestFile/people.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//读取Json文件，生成DataFrame：</span></span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印Schema结构信息：</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建临时视图：</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行查询</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age=18&quot;</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQ4ZGFmOTZlMmFiNDM4NmYucG5n?x-oss-process=image/format,png"></p>
<h4 id="5-使用JDBC-通过JDBC操作关系型数据库，mysql中的数据，通过JDBC加载到Spark中进行分析和处理"><a href="#5-使用JDBC-通过JDBC操作关系型数据库，mysql中的数据，通过JDBC加载到Spark中进行分析和处理" class="headerlink" title="5.使用JDBC(通过JDBC操作关系型数据库，mysql中的数据，通过JDBC加载到Spark中进行分析和处理)"></a>5.使用JDBC(通过JDBC操作关系型数据库，mysql中的数据，通过JDBC加载到Spark中进行分析和处理)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL同样支持通过JDBC读取其他数据库的数据作为数据源。</p>
<h4 id="一-Demo演示：使用Spark-SQL读取Oracle数据库中的表。"><a href="#一-Demo演示：使用Spark-SQL读取Oracle数据库中的表。" class="headerlink" title="(一)Demo演示：使用Spark SQL读取Oracle数据库中的表。"></a>(一)Demo演示：使用Spark SQL读取Oracle数据库中的表。</h4><h5 id="1-启动Spark-Shell的时候，指定Oracle数据库的驱动"><a href="#1-启动Spark-Shell的时候，指定Oracle数据库的驱动" class="headerlink" title="(1)启动Spark Shell的时候，指定Oracle数据库的驱动"></a>(1)启动Spark Shell的时候，指定Oracle数据库的驱动</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark://hadoop:7077 --jars /opt/soft/mysql-connector-java-5.1.27.jar --driver-class-path /opt/soft/mysql-connector-java-5.1.27.jar </span><br></pre></td></tr></table></figure>

<h5 id="2-读取mysql数据库中的数据"><a href="#2-读取mysql数据库中的数据" class="headerlink" title="(2)读取mysql数据库中的数据"></a>(2)读取mysql数据库中的数据</h5><h6 id="a-方式一："><a href="#a-方式一：" class="headerlink" title="(a)方式一："></a>(a)方式一：</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val mysqlDF = spark.read.format(<span class="string">&quot;jdbc&quot;</span>).</span><br><span class="line">         option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://192.168.1.120:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;</span>).</span><br><span class="line">         option(<span class="string">&quot;dbtable&quot;</span>,<span class="string">&quot;emp&quot;</span>).</span><br><span class="line">         option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>).</span><br><span class="line">         option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;000000&quot;</span>).load</span><br><span class="line"></span><br><span class="line">mysqlDF.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTUyMDhhNjViNGMxMGUxMTMucG5n?x-oss-process=image/format,png"></p>
<h6 id="b-方式二：定义-Properities类"><a href="#b-方式二：定义-Properities类" class="headerlink" title="(b)方式二：定义 Properities类"></a>(b)方式二：定义 Properities类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//导入需要的类：</span></span><br><span class="line"><span class="keyword">import</span> java.util.Properties   </span><br><span class="line"></span><br><span class="line"><span class="comment">//定义属性：               </span></span><br><span class="line">val mysqlProps = <span class="keyword">new</span> Properties()</span><br><span class="line">mysqlProps.setProperty(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>)</span><br><span class="line">mysqlProps.setProperty(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;000000&quot;</span>)</span><br><span class="line"><span class="comment">//读取数据：</span></span><br><span class="line"></span><br><span class="line">val mysqlDF1 = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://192.168.1.120:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;</span>,<span class="string">&quot;emp&quot;</span>,mysqlProps)</span><br><span class="line"></span><br><span class="line">mysqlDF1.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQ4NTE3N2Y3MTkyM2M4OGEucG5n?x-oss-process=image/format,png"></p>
<h4 id="6-使用Hive-Table-把Hive中的数据，读取到Spark-SQL-中"><a href="#6-使用Hive-Table-把Hive中的数据，读取到Spark-SQL-中" class="headerlink" title="6.使用Hive Table(把Hive中的数据，读取到Spark SQL 中)"></a>6.使用Hive Table(把Hive中的数据，读取到Spark SQL 中)</h4><h5 id="1-首先，搭建好Hive的环境-需要Hadoop"><a href="#1-首先，搭建好Hive的环境-需要Hadoop" class="headerlink" title="(1)首先，搭建好Hive的环境(需要Hadoop)"></a>(1)首先，搭建好Hive的环境(需要Hadoop)</h5><h6 id="a-搭建台Hive，一台Hive-Server-hadoop2-，一台Hive-Client-hadoop1"><a href="#a-搭建台Hive，一台Hive-Server-hadoop2-，一台Hive-Client-hadoop1" class="headerlink" title="(a)搭建台Hive，一台Hive Server(hadoop2)，一台Hive Client(hadoop1)"></a>(a)搭建台Hive，一台Hive Server(hadoop2)，一台Hive Client(hadoop1)</h6><p>这两台hive中其他配置文件一样，知识hive-site.xml有区别<br>其中Hive Server的hive-site.xml配置如下:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop1:3306/hive?serverTimezone=UTC<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive/iotmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.readOnlyDatastore<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.fixedDatastore<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.autoCreateSchema<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.autoCreateTables<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.autoCreateColumns<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Hive Client 中hive-site.xml配置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.1.122:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>    </span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTlmNDI3ZTM5NWJiMzJiOTAucG5n?x-oss-process=image/format,png" alt="Hive Client"></p>
<h5 id="2-配置Spark-SQL支持Hive"><a href="#2-配置Spark-SQL支持Hive" class="headerlink" title="(2)配置Spark SQL支持Hive"></a>(2)配置Spark SQL支持Hive</h5><h6 id="a-只需要将以下文件拷贝到-SPARK-HOME-conf的目录下，即可"><a href="#a-只需要将以下文件拷贝到-SPARK-HOME-conf的目录下，即可" class="headerlink" title="(a)只需要将以下文件拷贝到$SPARK_HOME/conf的目录下，即可"></a>(a)只需要将以下文件拷贝到$SPARK_HOME/conf的目录下，即可</h6><ul>
<li>$HIVE_HOME/conf/hive-site.xml(拷贝Hive Client中的hive-site.xml)</li>
<li>$HADOOP_CONF_DIR/core-site.xml</li>
<li>$HADOOP_CONF_DIR/hdfs-site.xml</li>
</ul>
<h5 id="3-启动hive："><a href="#3-启动hive：" class="headerlink" title="(3)启动hive："></a>(3)启动hive：</h5><p>启动Hive Server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hive-1.2.1</span><br><span class="line"></span><br><span class="line">bin/hive --service metastore</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTc4YTBjMGYzNmExOTk4Y2IucG5n?x-oss-process=image/format,png" alt="Hive Server"></p>
<p>启动Hive Client</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hive-1.2.1</span><br><span class="line"></span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTM1NTYzNDlhNzE3NTA5MzYucG5n?x-oss-process=image/format,png" alt="Hive Client"></p>
<h5 id="4-使用Spark-Shell操作Hive"><a href="#4-使用Spark-Shell操作Hive" class="headerlink" title="(4)使用Spark Shell操作Hive"></a>(4)使用Spark Shell操作Hive</h5><h6 id="a-启动Spark-Shell的时候，需要使用–jars指定mysql的驱动程序"><a href="#a-启动Spark-Shell的时候，需要使用–jars指定mysql的驱动程序" class="headerlink" title="(a)启动Spark Shell的时候，需要使用–jars指定mysql的驱动程序"></a>(a)启动Spark Shell的时候，需要使用–jars指定mysql的驱动程序</h6><p>启动Spark</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/spark-2.1.0-bin-hadoop2.7</span><br><span class="line"></span><br><span class="line">bin/spark-shell --master://hadoop1:7077</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from default.emp&quot;).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTc0ZGVkZGYyYWQ1ZTAxNDQucG5n?x-oss-process=image/format,png" alt="查询Hive中的表"></p>
<h6 id="b-创建表"><a href="#b-创建表" class="headerlink" title="(b)创建表"></a>(b)创建表</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;create table movle.src (key INT, value STRING) row format 	delimited fields terminated by &#x27;,&#x27;&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWFjODhmZWM1MjY2MzMyMzQucG5n?x-oss-process=image/format,png" alt="创建表1"></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTU4OGIxNzNmZDdkNGQyZmUucG5n?x-oss-process=image/format,png" alt="创建表2"></p>
<h6 id="c-导入数据"><a href="#c-导入数据" class="headerlink" title="(c )导入数据"></a>(c )导入数据</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;load data local path &#x27;/root/temp/data.txt&#x27; into table src&quot;)</span><br></pre></td></tr></table></figure>
<h6 id="d-查询数据"><a href="#d-查询数据" class="headerlink" title="(d)查询数据"></a>(d)查询数据</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;select * from src&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<h5 id="4-使用spark-sql操作Hive"><a href="#4-使用spark-sql操作Hive" class="headerlink" title="(4)使用spark-sql操作Hive"></a>(4)使用spark-sql操作Hive</h5><h6 id="a-启动spark-sql的时候，需要使用–jars指定mysql的驱动程序"><a href="#a-启动spark-sql的时候，需要使用–jars指定mysql的驱动程序" class="headerlink" title="(a)启动spark-sql的时候，需要使用–jars指定mysql的驱动程序"></a>(a)启动spark-sql的时候，需要使用–jars指定mysql的驱动程序</h6><h6 id="b-操作Hive"><a href="#b-操作Hive" class="headerlink" title="(b)操作Hive"></a>(b)操作Hive</h6><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;show tables&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE1ZjQxMThmNDY1NDVjYTEucG5n?x-oss-process=image/format,png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL/" rel="tag">Spark SQL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark SQL：基础"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/11/Spark%20SQL%EF%BC%9A%E5%9F%BA%E7%A1%80/"
    >Spark SQL:基础</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/11/Spark%20SQL%EF%BC%9A%E5%9F%BA%E7%A1%80/" class="article-date">
  <time datetime="2020-03-11T02:00:00.000Z" itemprop="datePublished">2020-03-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-Spark-SQL简介"><a href="#一-Spark-SQL简介" class="headerlink" title="一.Spark SQL简介"></a>一.Spark SQL简介</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>&nbsp;&nbsp;&nbsp;&nbsp;为什么要学习Spark SQL?Hive，它将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所以Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！同时Spark SQL也支持从Hive中读取数据。</p>
<h3 id="二-Spark-SQL的特点："><a href="#二-Spark-SQL的特点：" class="headerlink" title="二.Spark SQL的特点："></a>二.Spark SQL的特点：</h3><h4 id="1-容易整合-集成"><a href="#1-容易整合-集成" class="headerlink" title="1.容易整合(集成):"></a>1.容易整合(集成):</h4><p>安装Spark的时候，已经集成好了。不需要单独安装</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTk4MzQzODVlMGFmYzU4MjIucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-统一的数据访问方式"><a href="#2-统一的数据访问方式" class="headerlink" title="2.统一的数据访问方式"></a>2.统一的数据访问方式</h4><p>JDBC、JSON、Hive、parquet文件（一种列式存储文件，是SparkSQL默认的数据源）</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTc5MDhlMjI1Zjg5NTlkMzkucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-兼容Hive"><a href="#3-兼容Hive" class="headerlink" title="3.兼容Hive:"></a>3.兼容Hive:</h4><p>可以将Hive中的数据，直接读取到Spark SQL中处理。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBhZDA2ZTkyMjM5NzhlYzQucG5n?x-oss-process=image/format,png"></p>
<h4 id="4-标准的数据连接-JDBC"><a href="#4-标准的数据连接-JDBC" class="headerlink" title="4.标准的数据连接:JDBC"></a>4.标准的数据连接:JDBC</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTY4NWMxNThlZGEwYmZiZDYucG5n?x-oss-process=image/format,png"></p>
<h3 id="三-基本概念：表：Datasets和DataFrames"><a href="#三-基本概念：表：Datasets和DataFrames" class="headerlink" title="三.基本概念：表：Datasets和DataFrames"></a>三.基本概念：表：Datasets和DataFrames</h3><h4 id="1-表-表结构-数据"><a href="#1-表-表结构-数据" class="headerlink" title="1.表 = 表结构  +  数据"></a>1.表 = 表结构  +  数据</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;DataFrame = Schema(表结构) + RDD（代表数据）</p>
<h4 id="2-DataFrame"><a href="#2-DataFrame" class="headerlink" title="2.DataFrame"></a>2.DataFrame</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;DataFrame是组织成命名列的数据集。它在概念上等同于关系数据库中的表，但在底层具有更丰富的优化。DataFrames可以从各种来源构建，</p>
<p>例如：</p>
<ul>
<li>结构化数据文件</li>
<li>hive中的表</li>
<li>外部数据库或现有RDDs </li>
</ul>
<p>DataFrame API支持的语言有Scala，Java，Python和R</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTJlNTkwMTA2MTU3YzA3NGMucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;从上图可以看出，DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化</p>
<h4 id="3-Datasets"><a href="#3-Datasets" class="headerlink" title="3.Datasets"></a>3.Datasets</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。一个Dataset 可以从JVM对象构造，然后使用函数转换(map， flatMap，filter等)去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。</p>
<h3 id="四-创建DataFrames"><a href="#四-创建DataFrames" class="headerlink" title="四.创建DataFrames"></a>四.创建DataFrames</h3><h4 id="1-第一种方式：使用case-class样本类创建DataFrames"><a href="#1-第一种方式：使用case-class样本类创建DataFrames" class="headerlink" title="1.第一种方式：使用case class样本类创建DataFrames"></a>1.第一种方式：使用case class样本类创建DataFrames</h4><h5 id="1-定义表的Schema"><a href="#1-定义表的Schema" class="headerlink" title="(1)定义表的Schema"></a>(1)定义表的Schema</h5><p>注意：由于mgr和comm列中包含null值，简单起见，将对应的case class类型定义为String</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">case</span> class <span class="title">Emp</span><span class="params">(empno:Int,ename:String,job:String,mgr:String,hiredate:String,sal:Int,comm:String,depno:Int)</span></span></span><br></pre></td></tr></table></figure>

<h5 id="2-读入数据"><a href="#2-读入数据" class="headerlink" title="(2)读入数据"></a>(2)读入数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从hdfs中读入</span></span><br><span class="line">scala&gt; val lines = sc.textFile(<span class="string">&quot;hdfs://hadoop1:9000/emp.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//从本地读入</span></span><br><span class="line">scala&gt; val lines = sc.textFile(<span class="string">&quot;/opt/module/datas/TestFile/emp.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>/opt/module/datas/TestFile </p>
<h5 id="3-把每行数据映射到Emp中。把表结构和数据，关联。"><a href="#3-把每行数据映射到Emp中。把表结构和数据，关联。" class="headerlink" title="(3)把每行数据映射到Emp中。把表结构和数据，关联。"></a>(3)把每行数据映射到Emp中。把表结构和数据，关联。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val allEmp = lines.map(x =&gt; Emp(x(<span class="number">0</span>).toInt,x(<span class="number">1</span>),x(<span class="number">2</span>),x(<span class="number">3</span>),x(<span class="number">4</span>),x(<span class="number">5</span>).toInt,x(<span class="number">6</span>),x(<span class="number">7</span>).toInt))</span><br></pre></td></tr></table></figure>
<h5 id="4-生成DataFrame"><a href="#4-生成DataFrame" class="headerlink" title="(4)生成DataFrame"></a>(4)生成DataFrame</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val allEmpDF = allEmp.toDF</span><br><span class="line"></span><br><span class="line"><span class="comment">//展示 </span></span><br><span class="line">scala&gt; allEmpDF.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWFmZDU1NTU0Yzc3Nzc3ZDIucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-第二种方式：使用SparkSession"><a href="#2-第二种方式：使用SparkSession" class="headerlink" title="2.第二种方式：使用SparkSession"></a>2.第二种方式：使用SparkSession</h4><h5 id="1-什么是SparkSession"><a href="#1-什么是SparkSession" class="headerlink" title="(1)什么是SparkSession"></a>(1)什么是SparkSession</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;Apache Spark 2.0引入了SparkSession，其为用户提供了一个统一的切入点来使用Spark的各项功能，并且允许用户通过它调用DataFrame和Dataset相关API来编写Spark程序。最重要的是，它减少了用户需要了解的一些概念，使得我们可以很容易地与Spark交互。<br>&nbsp;&nbsp;&nbsp;&nbsp;在2.0版本之前，与Spark交互之前必须先创建SparkConf和SparkContext。然而在Spark 2.0中，我们可以通过SparkSession来实现同样的功能，而不需要显式地创建SparkConf, SparkContext 以及 SQLContext，因为这些对象已经封装在SparkSession中。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTlhMDk4MDMzYjY3OTI0N2MucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-使用StructType，来创建Schema"><a href="#2-使用StructType，来创建Schema" class="headerlink" title="(2)使用StructType，来创建Schema"></a>(2)使用StructType，来创建Schema</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">val myschema = StructType(</span><br><span class="line">				List(</span><br><span class="line">				StructField(<span class="string">&quot;empno&quot;</span>, DataTypes.IntegerType), </span><br><span class="line">				StructField(<span class="string">&quot;ename&quot;</span>, DataTypes.StringType),</span><br><span class="line">				StructField(<span class="string">&quot;job&quot;</span>, DataTypes.StringType),</span><br><span class="line">				StructField(<span class="string">&quot;mgr&quot;</span>, DataTypes.IntegerType),</span><br><span class="line">				StructField(<span class="string">&quot;hiredate&quot;</span>, DataTypes.StringType),</span><br><span class="line">				StructField(<span class="string">&quot;sal&quot;</span>, DataTypes.IntegerType),</span><br><span class="line">				StructField(<span class="string">&quot;comm&quot;</span>, DataTypes.IntegerType),</span><br><span class="line">				StructField(<span class="string">&quot;deptno&quot;</span>, DataTypes.IntegerType)))</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWNlYjM3MGNmYjk0ZDg5NmMucG5n?x-oss-process=image/format,png"></p>
<p><strong>注意，需要：import org.apache.spark.sql.types._</strong></p>
<h5 id="3-读取文件："><a href="#3-读取文件：" class="headerlink" title="(3)读取文件："></a>(3)读取文件：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lines= sc.textFile(<span class="string">&quot;/opt/module/datas/TestFile/emp.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LThjNzZjOTY2YzY3OTZiM2MucG5n?x-oss-process=image/format,png"></p>
<h5 id="4-数据与表结构匹配"><a href="#4-数据与表结构匹配" class="headerlink" title="(4)数据与表结构匹配"></a>(4)数据与表结构匹配</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row</span><br><span class="line"></span><br><span class="line">val allEmp = lines.map(x =&gt; Row(x(<span class="number">0</span>).toInt,x(<span class="number">1</span>),x(<span class="number">2</span>),x(<span class="number">3</span>).toInt,x(<span class="number">4</span>),x(<span class="number">5</span>).toInt,x(<span class="number">6</span>).toInt,x(<span class="number">7</span>).toInt))</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTVmMTA5ZTFmZmU1YTIwYzEucG5n?x-oss-process=image/format,png"></p>
<p><strong>注意，需要：import org.apache.spark.sql.Row</strong></p>
<h5 id="5-创建DataFrames"><a href="#5-创建DataFrames" class="headerlink" title="(5)创建DataFrames"></a>(5)创建DataFrames</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df2 = spark.createDataFrame(allEmp,myschema)</span><br><span class="line"></span><br><span class="line">df2.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWEyODQwODNhOTJhMjJjNjAucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-方式三，直接读取一个带格式的文件：Json"><a href="#3-方式三，直接读取一个带格式的文件：Json" class="headerlink" title="3.方式三，直接读取一个带格式的文件：Json"></a>3.方式三，直接读取一个带格式的文件：Json</h4><h5 id="1-读取文件："><a href="#1-读取文件：" class="headerlink" title="(1)读取文件："></a>(1)读取文件：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df3 = spark.read.json(<span class="string">&quot;/opt/module/datas/TestFile/emp.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">df3.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTYxYTRmOWZhZTY5M2VmMTUucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-另一种方式"><a href="#2-另一种方式" class="headerlink" title="(2)另一种方式"></a>(2)另一种方式</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df4 = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/opt/module/datas/TestFile/emp.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">df4.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTgyZGE4NTUyMDMyMjdmZjIucG5n?x-oss-process=image/format,png"></p>
<h3 id="五-操作DataFrame"><a href="#五-操作DataFrame" class="headerlink" title="五.操作DataFrame"></a>五.操作DataFrame</h3><p>DataFrame操作也称为无类型的Dataset操作</p>
<h4 id="1-DSL语句"><a href="#1-DSL语句" class="headerlink" title="1.DSL语句"></a>1.DSL语句</h4><h5 id="1"><a href="#1" class="headerlink" title="(1)"></a>(1)</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df1.show</span><br><span class="line"></span><br><span class="line">df1.printSchema</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBlZGU0OWE4MTg5YzcyMGMucG5n?x-oss-process=image/format,png"></p>
<h5 id="2"><a href="#2" class="headerlink" title="(2)"></a>(2)</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df1.select(<span class="string">&quot;ename&quot;</span>,<span class="string">&quot;sal&quot;</span>).show</span><br><span class="line"></span><br><span class="line">df1.select($<span class="string">&quot;ename&quot;</span>,$<span class="string">&quot;sal&quot;</span>,$<span class="string">&quot;sal&quot;</span>+<span class="number">100</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTA5ZDc4ZTZkOTkxOWFjNTAucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-代表-取出来以后，再做一些操作"><a href="#3-代表-取出来以后，再做一些操作" class="headerlink" title="(3)$代表 取出来以后，再做一些操作"></a>(3)$代表 取出来以后，再做一些操作</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df1.filter($<span class="string">&quot;sal&quot;</span>&gt;<span class="number">2000</span>).show</span><br><span class="line"></span><br><span class="line">df1.groupBy($<span class="string">&quot;depno&quot;</span>).count.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTFmYjUwNjNhMTVkN2Y3ZTQucG5n?x-oss-process=image/format,png"></p>
<p><strong>完整的例子，请参考：</strong><br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset">http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset</a></p>
<h4 id="2-SQL语句"><a href="#2-SQL语句" class="headerlink" title="2.SQL语句"></a>2.SQL语句</h4><p>注意：不能直接执行sql。需要生成一个视图，再执行SQL。</p>
<h5 id="1-将DataFrame注册成表-视图-："><a href="#1-将DataFrame注册成表-视图-：" class="headerlink" title="(1)将DataFrame注册成表(视图)："></a>(1)将DataFrame注册成表(视图)：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.createOrReplaceTempView(<span class="string">&quot;emp&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="2-执行查询："><a href="#2-执行查询：" class="headerlink" title="(2)执行查询："></a>(2)执行查询：</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;select * from emp&quot;).<span class="keyword">show</span></span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from emp where sal &gt; 2000&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBlOGVjNjQyMzMxYjIwY2IucG5n?x-oss-process=image/format,png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;select * from emp where depno=10&quot;).<span class="keyword">show</span></span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select depno,count(1) from emp group by depno&quot;).<span class="keyword">show</span> </span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select depno,sum(sal) from emp group by depno&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTgwMGI1MzllMTFhZjIxMTcucG5n?x-oss-process=image/format,png" alt="image.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df1.createOrReplaceTempView(&quot;emp12345&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select e.depno from emp12345 e&quot;).<span class="keyword">show</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTY1MzU2NDNlOGIyNzE2NTYucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-多表查询"><a href="#3-多表查询" class="headerlink" title="3.多表查询"></a>3.多表查询</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Dept</span><span class="params">(deptno:Int,dname:String,loc:String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">val lines </span>= sc.textFile(<span class="string">&quot;/opt/module/datas/TestFile/dept.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">val allDept = lines.map(x=&gt;Dept(x(<span class="number">0</span>).toInt,x(<span class="number">1</span>),x(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">val df2 = allDept.toDF</span><br><span class="line"></span><br><span class="line">df2.create</span><br><span class="line"></span><br><span class="line">df2.createOrReplaceTempView(<span class="string">&quot;dept&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select dname,ename from emp12345,dept where emp12345.depno=dept.deptno&quot;</span>).show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE4MWMzNGNmNTdkNzUwZDQucG5n?x-oss-process=image/format,png"></p>
<h3 id="六-视图"><a href="#六-视图" class="headerlink" title="六.视图"></a>六.视图</h3><h4 id="1-视图是一个虚表，不存储数据"><a href="#1-视图是一个虚表，不存储数据" class="headerlink" title="1.视图是一个虚表，不存储数据"></a>1.视图是一个虚表，不存储数据</h4><h4 id="2-两种类型视图："><a href="#2-两种类型视图：" class="headerlink" title="2.两种类型视图："></a>2.两种类型视图：</h4><h5 id="1-普通视图（本地视图）：只在当前Session有效"><a href="#1-普通视图（本地视图）：只在当前Session有效" class="headerlink" title="(1)普通视图（本地视图）：只在当前Session有效"></a>(1)普通视图（本地视图）：只在当前Session有效</h5><h5 id="2-全局视图：在不同Session中都有用。全局视图创建在命名空间中：global-temp-类似于一个库。"><a href="#2-全局视图：在不同Session中都有用。全局视图创建在命名空间中：global-temp-类似于一个库。" class="headerlink" title="(2)全局视图：在不同Session中都有用。全局视图创建在命名空间中：global_temp 类似于一个库。"></a>(2)全局视图：在不同Session中都有用。全局视图创建在命名空间中：global_temp 类似于一个库。</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;上面使用的是一个在Session生命周期中的临时views。在Spark SQL中，如果你想拥有一个临时的view，并想在不同的Session中共享，而且在application的运行周期内可用，那么就需要创建一个全局的临时view。并记得使用的时候加上global_temp作为前缀来引用它，因为全局的临时view是绑定到系统保留的数据库global_temp上。</p>
<h6 id="a-创建一个普通的view和一个全局的view"><a href="#a-创建一个普通的view和一个全局的view" class="headerlink" title="(a)创建一个普通的view和一个全局的view"></a>(a)创建一个普通的view和一个全局的view</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df1.createOrReplaceTempView(<span class="string">&quot;emp1&quot;</span>)</span><br><span class="line"></span><br><span class="line">df1.createGlobalTempView(<span class="string">&quot;emp2&quot;</span>)</span><br></pre></td></tr></table></figure>
<h6 id="b-在当前会话中执行查询，均可查询出结果。"><a href="#b-在当前会话中执行查询，均可查询出结果。" class="headerlink" title="(b)在当前会话中执行查询，均可查询出结果。"></a>(b)在当前会话中执行查询，均可查询出结果。</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">&quot;select * from emp1&quot;</span>).show</span><br><span class="line">spark.sql(<span class="string">&quot;select * from global_temp.emp2&quot;</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTVkMTlkYmNjOWFhZDliNWUucG5n?x-oss-process=image/format,png"></p>
<h6 id="c-开启一个新的会话，执行同样的查询"><a href="#c-开启一个新的会话，执行同样的查询" class="headerlink" title="(c )开启一个新的会话，执行同样的查询"></a>(c )开启一个新的会话，执行同样的查询</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.newSession.sql(<span class="string">&quot;select * from emp1&quot;</span>).show     <span class="comment">//（运行出错）</span></span><br><span class="line">spark.newSession.sql(<span class="string">&quot;select * from global_temp.emp2&quot;</span>).show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWE0ZWJlMDhmMDgyOGYxYTEucG5n?x-oss-process=image/format,png"></p>
<h3 id="七-创建Datasets"><a href="#七-创建Datasets" class="headerlink" title="七.创建Datasets"></a>七.创建Datasets</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;DataFrame的引入，可以让Spark更好的处理结构数据的计算，但其中一个主要的问题是：缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API (DataFrame API的类型扩展)。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LThjYzRmNDFhMjA2ZjNmMDIucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点(强类型，可以使用功能强大的lambda)以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)</p>
<h4 id="1-方式一：使用序列"><a href="#1-方式一：使用序列" class="headerlink" title="1.方式一：使用序列"></a>1.方式一：使用序列</h4><h5 id="1-定义case-class"><a href="#1-定义case-class" class="headerlink" title="(1)定义case class"></a>(1)定义case class</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;<span class="function"><span class="keyword">case</span> class <span class="title">MyData</span><span class="params">(a:Int,b:String)</span></span></span><br></pre></td></tr></table></figure>
<h5 id="2-生成序列，并创建DataSet"><a href="#2-生成序列，并创建DataSet" class="headerlink" title="(2).生成序列，并创建DataSet"></a>(2).生成序列，并创建DataSet</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;val ds = Seq(MyData(<span class="number">1</span>,<span class="string">&quot;Tom&quot;</span>),MyData(<span class="number">2</span>,<span class="string">&quot;Mary&quot;</span>)).toDS</span><br></pre></td></tr></table></figure>
<h5 id="3-查看结果"><a href="#3-查看结果" class="headerlink" title="(3).查看结果"></a>(3).查看结果</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;ds.show</span><br><span class="line"></span><br><span class="line">ds.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTg0ZDk0ZDE2ZjIxNzhiZDEucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-方式二：使用JSON数据"><a href="#2-方式二：使用JSON数据" class="headerlink" title="2.方式二：使用JSON数据"></a>2.方式二：使用JSON数据</h4><h5 id="1-定义case-class-1"><a href="#1-定义case-class-1" class="headerlink" title="(1)定义case class"></a>(1)定义case class</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Person</span><span class="params">(name: String, age: BigInt)</span></span></span><br></pre></td></tr></table></figure>
<h5 id="2-通过JSON数据生成DataFrame"><a href="#2-通过JSON数据生成DataFrame" class="headerlink" title="(2)通过JSON数据生成DataFrame"></a>(2)通过JSON数据生成DataFrame</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/opt/module/datas/TestFile/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="3-将DataFrame转成DataSet"><a href="#3-将DataFrame转成DataSet" class="headerlink" title="(3)将DataFrame转成DataSet"></a>(3)将DataFrame转成DataSet</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.as[Person].show</span><br><span class="line">df.as[Person].collect</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRhODZlZjFjOGY0MGNhMGQucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-方式三：使用其他数据-RDD的操作和DataFrame操作结合"><a href="#3-方式三：使用其他数据-RDD的操作和DataFrame操作结合" class="headerlink" title="3.方式三：使用其他数据(RDD的操作和DataFrame操作结合)"></a>3.方式三：使用其他数据(RDD的操作和DataFrame操作结合)</h4><h5 id="1-需求：分词；查询出长度大于3的单词"><a href="#1-需求：分词；查询出长度大于3的单词" class="headerlink" title="(1)需求：分词；查询出长度大于3的单词"></a>(1)需求：分词；查询出长度大于3的单词</h5><h6 id="a-读取数据，并创建DataSet"><a href="#a-读取数据，并创建DataSet" class="headerlink" title="(a)读取数据，并创建DataSet"></a>(a)读取数据，并创建DataSet</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val linesDS = spark.read.text(<span class="string">&quot;/opt/module/datas/TestFile/test_WordCount.txt&quot;</span>).as[String]</span><br></pre></td></tr></table></figure>
<h6 id="b-对DataSet进行操作：分词后，查询长度大于3的单词"><a href="#b-对DataSet进行操作：分词后，查询长度大于3的单词" class="headerlink" title="(b)对DataSet进行操作：分词后，查询长度大于3的单词"></a>(b)对DataSet进行操作：分词后，查询长度大于3的单词</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val words = linesDS.flatMap(_.split(<span class="string">&quot; &quot;</span>)).filter(_.length &gt; <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">words.show</span><br><span class="line"></span><br><span class="line">words.collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQzMDUzNzIwMDAwMjhmN2IucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-需求：执行WordCount程序"><a href="#2-需求：执行WordCount程序" class="headerlink" title="(2)需求：执行WordCount程序"></a>(2)需求：执行WordCount程序</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = linesDS.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,<span class="number">1</span>)).groupByKey(x =&gt; x._1).count</span><br><span class="line"></span><br><span class="line">result.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTc2YmNkYzM3NDJmNjJiMTkucG5n?x-oss-process=image/format,png"></p>
<p>排序：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result.orderBy($<span class="string">&quot;value&quot;</span>).show</span><br><span class="line"></span><br><span class="line">result.orderBy($<span class="string">&quot;count(1)&quot;</span>).show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQ2MDljMjVhMGM0NzRkNDYucG5n?x-oss-process=image/format,png"></p>
<h3 id="八-Datasets的操作案例"><a href="#八-Datasets的操作案例" class="headerlink" title="八.Datasets的操作案例"></a>八.Datasets的操作案例</h3><h4 id="1-使用emp-json-生成DataFrame"><a href="#1-使用emp-json-生成DataFrame" class="headerlink" title="1.使用emp.json 生成DataFrame"></a>1.使用emp.json 生成DataFrame</h4><h5 id="1-数据：emp-json"><a href="#1-数据：emp-json" class="headerlink" title="(1)数据：emp.json"></a>(1)数据：emp.json</h5><h5 id="2-使用emp-json-生成DataFrame"><a href="#2-使用emp-json-生成DataFrame" class="headerlink" title="(2)使用emp.json 生成DataFrame"></a>(2)使用emp.json 生成DataFrame</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val empDF = spark.read.json(<span class="string">&quot;/opt/module/datas/TestFile/emp.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">emp.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTk5N2RlYWQyNTFhZjAxMWMucG5n?x-oss-process=image/format,png"></p>
<p>查询工资大于3000的员工</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.where($<span class="string">&quot;sal&quot;</span> &gt;= <span class="number">3000</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWY3NjdmZDNmNDA2YjczZGEucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-创建case-class-生成DataSets"><a href="#3-创建case-class-生成DataSets" class="headerlink" title="(3)创建case class,生成DataSets"></a>(3)创建case class,生成DataSets</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Emp</span><span class="params">(empno:Long,ename:String,job:String,hiredate:String,mgr:String,sal:Long,comm:String,deptno:Long)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">val empDS </span>= empDF.as[Emp]</span><br></pre></td></tr></table></figure>
<h5 id="4-查询数据"><a href="#4-查询数据" class="headerlink" title="(4)查询数据"></a>(4)查询数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查询工资大于3000的员工</span></span><br><span class="line">empDS.filter(_.sal &gt; <span class="number">3000</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">//查看10号部门的员工 </span></span><br><span class="line">empDS.filter(_.deptno == <span class="number">10</span>).show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQ3OTE4MTZlZGEzMzllMGEucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-多表查询"><a href="#2-多表查询" class="headerlink" title="2.多表查询"></a>2.多表查询</h4><h5 id="1-创建部门表"><a href="#1-创建部门表" class="headerlink" title="(1)创建部门表"></a>(1)创建部门表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val deptRDD=sc.textFile(<span class="string">&quot;/opt/module/datas/TestFile/dept.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Dept</span><span class="params">(deptno:Int,dname:String,loc:String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">val deptDS </span>= deptRDD.map(x=&gt;Dept(x(<span class="number">0</span>).toInt,x(<span class="number">1</span>),x(<span class="number">2</span>))).toDS</span><br><span class="line"></span><br><span class="line">deptDS.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTAzMzMzMmUwMzMzZjM0YmIucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-创建员工表"><a href="#2-创建员工表" class="headerlink" title="(2)创建员工表"></a>(2)创建员工表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Emp</span><span class="params">(empno:Int,ename:String,job:String,mgr:String,hiredate:String,sal:Int,comm:String,deptno:Int)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">val empRDD </span>= sc.textFile(<span class="string">&quot;/opt/module/datas/TestFile/emp.csv&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">val empDS = empRDD.map(x =&gt; Emp(x(<span class="number">0</span>).toInt,x(<span class="number">1</span>),x(<span class="number">2</span>),x(<span class="number">3</span>),x(<span class="number">4</span>),x(<span class="number">5</span>).toInt,x(<span class="number">6</span>),x(<span class="number">7</span>).toInt)).toDS</span><br><span class="line"></span><br><span class="line">empDS.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTNkNDhiNTMyMjFkNWIyNzYucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-执行多表查询：等值链接"><a href="#3-执行多表查询：等值链接" class="headerlink" title="(3)执行多表查询：等值链接"></a>(3)执行多表查询：等值链接</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = deptDS.join(empDS,<span class="string">&quot;deptno&quot;</span>)</span><br><span class="line"></span><br><span class="line">result.show</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWEyNGIyNWM1NjIyYzU2MzYucG5n?x-oss-process=image/format,png"></p>
<h5 id="4-另一种写法：注意有三个等号"><a href="#4-另一种写法：注意有三个等号" class="headerlink" title="(4)另一种写法：注意有三个等号"></a>(4)另一种写法：注意有三个等号</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result1 = deptDS.joinWith(empDS,deptDS(<span class="string">&quot;deptno&quot;</span>)=== empDS(<span class="string">&quot;deptno&quot;</span>))</span><br><span class="line"></span><br><span class="line">result1.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBiNjgxZTBiZTY2NjQ1NmQucG5n?x-oss-process=image/format,png"></p>
<p>joinWith和join的区别是连接后的新Dataset的schema会不一样</p>
<h5 id="5-多表条件查询："><a href="#5-多表条件查询：" class="headerlink" title="(5)多表条件查询："></a>(5)多表条件查询：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = deptDS.join(empDS,<span class="string">&quot;deptno&quot;</span>).where(<span class="string">&quot;deptno==10&quot;</span>) </span><br><span class="line"></span><br><span class="line">result.show</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTk4NWJiYjVjMDZkYmU1ZDIucG5n?x-oss-process=image/format,png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-SQL/" rel="tag">Spark SQL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark Streaming：性能优化"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"
    >Spark Streaming：性能优化</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" class="article-date">
  <time datetime="2020-03-10T12:00:00.000Z" itemprop="datePublished">2020-03-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-减少批数据的执行时间"><a href="#1-减少批数据的执行时间" class="headerlink" title="1.减少批数据的执行时间"></a>1.减少批数据的执行时间</h4><p>在Spark中有几个优化可以减少批处理的时间：</p>
<h5 id="1-数据接收的并行水平"><a href="#1-数据接收的并行水平" class="headerlink" title="(1)数据接收的并行水平"></a>(1)数据接收的并行水平</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;通过网络(如kafka，flume，socket等)接收数据需要这些数据反序列化并被保存到Spark中。如果数据接收成为系统的瓶颈，就要考虑并行地接收数据。注意，每个输入DStream创建一个receiver(运行在worker机器上)接收单个数据流。创建多个输入DStream并配置它们可以从源中接收不同分区的数据流，从而实现多数据流接收。例如，接收两个topic数据的单个输入DStream可以被切分为两个kafka输入流，每个接收一个topic。这将在两个worker上运行两个receiver，因此允许数据并行接收，提高整体的吞吐量。多个DStream可以被合并生成单个DStream，这样运用在单个输入DStream的transformation操作可以运用在合并的DStream上。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTM1MmI1NDBmMjBiNjk4ZDIucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-数据处理的并行水平"><a href="#2-数据处理的并行水平" class="headerlink" title="(2)数据处理的并行水平"></a>(2)数据处理的并行水平</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;如果运行在计算stage上的并发任务数不足够大，就不会充分利用集群的资源。默认的并发任务数通过配置属性来确定spark.default.parallelism。</p>
<h5 id="3-数据序列化"><a href="#3-数据序列化" class="headerlink" title="(3)数据序列化"></a>(3)数据序列化</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;可以通过改变序列化格式来减少数据序列化的开销。在流式传输的情况下，有两种类型的数据会被序列化：</p>
<ul>
<li>输入数据</li>
<li>由流操作生成的持久RDD</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;在上述两种情况下，使用Kryo序列化格式可以减少CPU和内存开销。</p>
<h4 id="2-设置正确的批容量"><a href="#2-设置正确的批容量" class="headerlink" title="2.设置正确的批容量"></a>2.设置正确的批容量</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;为了Spark Streaming应用程序能够在集群中稳定运行，系统应该能够以足够的速度处理接收的数据（即处理速度应该大于或等于接收数据的速度）。这可以通过流的网络UI观察得到。批处理时间应该小于批间隔时间。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;根据流计算的性质，批间隔时间可能显著的影响数据处理速率，这个速率可以通过应用程序维持。可以考虑WordCountNetwork这个例子，对于一个特定的数据处理速率，系统可能可以每2秒打印一次单词计数(批间隔时间为2秒)，但无法每500毫秒打印一次单词计数。所以，为了在生产环境中维持期望的数据处理速率，就应该设置合适的批间隔时间(即批数据的容量)。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;找出正确的批容量的一个好的办法是用一个保守的批间隔时间（5-10,秒）和低数据速率来测试你的应用程序。</p>
<h4 id="3-内存调优"><a href="#3-内存调优" class="headerlink" title="3.内存调优"></a>3.内存调优</h4><p>在这一节，重点介绍几个强烈推荐的自定义选项，它们可以减少Spark Streaming应用程序垃圾回收的相关暂停，获得更稳定的批处理时间。</p>
<h5 id="1-Default-persistence-level-of-DStreams："><a href="#1-Default-persistence-level-of-DStreams：" class="headerlink" title="(1)Default persistence level of DStreams："></a>(1)Default persistence level of DStreams：</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;和RDDs不同的是，默认的持久化级别是序列化数据到内存中(DStream是StorageLevel.MEMORY_ONLY_SER，RDD是StorageLevel.MEMORY_ONLY)。即使保存数据为序列化形态会增加序列化/反序列化的开销，但是可以明显的减少垃圾回收的暂停。</p>
<h5 id="2-Clearing-persistent-RDDs："><a href="#2-Clearing-persistent-RDDs：" class="headerlink" title="(2)Clearing persistent RDDs："></a>(2)Clearing persistent RDDs：</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，通过Spark内置策略(LUR)，Spark Streaming生成的持久化RDD将会从内存中清理掉。如果spark.cleaner.ttl已经设置了，比这个时间存在更老的持久化RDD将会被定时的清理掉。正如前面提到的那样，这个值需要根据Spark Streaming应用程序的操作小心设置。然而，可以设置配置选项spark.streaming.unpersist为true来更智能的去持久化(unpersist)RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为。</p>
<h5 id="3-Concurrent-garbage-collector："><a href="#3-Concurrent-garbage-collector：" class="headerlink" title="(3)Concurrent garbage collector："></a>(3)Concurrent garbage collector：</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;使用并发的标记-清除垃圾回收可以进一步减少垃圾回收的暂停时间。尽管并发的垃圾回收会减少系统的整体吞吐量，但是仍然推荐使用它以获得更稳定的批处理时间。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-Streaming/" rel="tag">Spark Streaming</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark Streaming：高级数据源"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90/"
    >Spark Streaming：高级数据源</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90/" class="article-date">
  <time datetime="2020-03-10T11:00:00.000Z" itemprop="datePublished">2020-03-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-Spark-Streaming接收Flume数据"><a href="#一-Spark-Streaming接收Flume数据" class="headerlink" title="一.Spark Streaming接收Flume数据"></a>一.Spark Streaming接收Flume数据</h3><h4 id="1-基于Flume的Push模式"><a href="#1-基于Flume的Push模式" class="headerlink" title="1.基于Flume的Push模式"></a>1.基于Flume的Push模式</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Flume被用于在Flume agents之间推送数据.在这种方式下,Spark Streaming可以很方便的建立一个receiver,起到一个Avro agent的作用.Flume可以将数据推送到改receiver.</p>
<h5 id="1-第一步：Flume的配置文件"><a href="#1-第一步：Flume的配置文件" class="headerlink" title="(1)第一步：Flume的配置文件"></a>(1)第一步：Flume的配置文件</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRlNTIwM2ViNTFjMDg0YTUucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-第二步：Spark-Streaming程序"><a href="#2-第二步：Spark-Streaming程序" class="headerlink" title="(2)第二步：Spark Streaming程序"></a>(2)第二步：Spark Streaming程序</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTkyODRiZGZkNWIxZjA3N2EucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-第三步：注意除了需要使用Flume的lib的jar包以外，还需要以下jar包："><a href="#3-第三步：注意除了需要使用Flume的lib的jar包以外，还需要以下jar包：" class="headerlink" title="(3)第三步：注意除了需要使用Flume的lib的jar包以外，还需要以下jar包："></a>(3)第三步：注意除了需要使用Flume的lib的jar包以外，还需要以下jar包：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-streaming-flume_2.1.0.jar</span><br></pre></td></tr></table></figure>

<h5 id="4-第四步：测试"><a href="#4-第四步：测试" class="headerlink" title="(4)第四步：测试"></a>(4)第四步：测试</h5><ul>
<li>启动Spark Streaming程序</li>
<li>启动Flume</li>
<li>拷贝日志文件到/root/training/logs目录</li>
<li>观察输出，采集到数据</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQxZDNjMmZjOGU0YzNjYzAucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-基于Custom-Sink的Pull模式"><a href="#2-基于Custom-Sink的Pull模式" class="headerlink" title="2.基于Custom Sink的Pull模式"></a>2.基于Custom Sink的Pull模式</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;不同于Flume直接将数据推送到Spark Streaming中，第二种模式通过以下条件运行一个正常的Flume sink。Flume将数据推送到sink中，并且数据保持buffered状态。Spark Streaming使用一个可靠的Flume接收器和转换器从sink拉取数据。只要当数据被接收并且被Spark Streaming备份后，转换器才运行成功。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;这样,与第一种模式相比,保证了很好的健壮性和容错能力。然而,这种模式需要为Flume配置一个正常的sink。</p>
<p>以下为配置步骤：</p>
<h5 id="1-第一步：Flume的配置文件-1"><a href="#1-第一步：Flume的配置文件-1" class="headerlink" title="(1)第一步：Flume的配置文件"></a>(1)第一步：Flume的配置文件</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTVhYzFiYzk3MDI0MTliMTMucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-第二步：Spark-Streaming程序-1"><a href="#2-第二步：Spark-Streaming程序-1" class="headerlink" title="(2)第二步：Spark Streaming程序"></a>(2)第二步：Spark Streaming程序</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTZjODhkMjAyNTVhMzIxNGEucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-第三步：需要的jar包"><a href="#3-第三步：需要的jar包" class="headerlink" title="(3)第三步：需要的jar包"></a>(3)第三步：需要的jar包</h5><ul>
<li>将Spark的jar包拷贝到Flume的lib目录下</li>
<li>下面的这个jar包也需要拷贝到Flume的lib目录下，同时加入IDEA工程的classpath</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-streaming-flume-sink_2.1.0.jar</span><br></pre></td></tr></table></figure>


<h5 id="4-第四步：测试-1"><a href="#4-第四步：测试-1" class="headerlink" title="(4)第四步：测试"></a>(4)第四步：测试</h5><ul>
<li>启动Flume</li>
<li>在IDEA中启动FlumeLogPull</li>
<li>将测试数据拷贝到/root/training/logs</li>
<li>观察IDEA中的输出</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWI5OTY2YjU2NTE0MGZmNzcucG5n?x-oss-process=image/format,png"></p>
<h3 id="二-Spark-Streaming接收Kafka数据"><a href="#二-Spark-Streaming接收Kafka数据" class="headerlink" title="二.Spark Streaming接收Kafka数据"></a>二.Spark Streaming接收Kafka数据</h3><p>Apache Kafka是一种高吞吐量的分布式发布订阅消息系统。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWI3NTNmODdiN2VhZjM1ODUucG5n?x-oss-process=image/format,png" alt="Kafka"></p>
<h4 id="1-搭建ZooKeeper（Standalone）："><a href="#1-搭建ZooKeeper（Standalone）：" class="headerlink" title="1.搭建ZooKeeper（Standalone）："></a>1.搭建ZooKeeper（Standalone）：</h4><h5 id="1-配置-root-training-zookeeper-3-4-10-conf-zoo-cfg文件"><a href="#1-配置-root-training-zookeeper-3-4-10-conf-zoo-cfg文件" class="headerlink" title="(1)配置/root/training/zookeeper-3.4.10/conf/zoo.cfg文件"></a>(1)配置/root/training/zookeeper-3.4.10/conf/zoo.cfg文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/root/training/zookeeper-3.4.10/tmp</span><br><span class="line">server.1=spark81:2888:3888</span><br></pre></td></tr></table></figure>
<h5 id="2-在-root-training-zookeeper-3-4-10-tmp目录下创建一个myid的空文件"><a href="#2-在-root-training-zookeeper-3-4-10-tmp目录下创建一个myid的空文件" class="headerlink" title="(2)在/root/training/zookeeper-3.4.10/tmp目录下创建一个myid的空文件"></a>(2)在/root/training/zookeeper-3.4.10/tmp目录下创建一个myid的空文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /root/training/zookeeper-3.4.6/tmp/myid</span><br></pre></td></tr></table></figure>
<h4 id="2-搭建Kafka环境-单机单broker-："><a href="#2-搭建Kafka环境-单机单broker-：" class="headerlink" title="2.搭建Kafka环境(单机单broker)："></a>2.搭建Kafka环境(单机单broker)：</h4><h5 id="1-修改server-properties文件"><a href="#1-修改server-properties文件" class="headerlink" title="(1)修改server.properties文件"></a>(1)修改server.properties文件</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWExZTJkMzQ5NzYzMzIxYzYucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-启动Kafka"><a href="#2-启动Kafka" class="headerlink" title="(2)启动Kafka"></a>(2)启动Kafka</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>
<p> 出现以下错误：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTUxYWFjZjM2OTJmOTVjMmQucG5n?x-oss-process=image/format,png"></p>
<p> 需要修改bin/kafka-run-class.sh文件，将这个选项注释掉。</p>
<h5 id="3-测试Kafka"><a href="#3-测试Kafka" class="headerlink" title="(3)测试Kafka"></a>(3)测试Kafka</h5><ul>
<li>创建Topic<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper spark81:2181 -replication-factor 1 --partitions 3 --topic mydemo1</span><br></pre></td></tr></table></figure></li>
<li>发送消息<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list spark81:9092 --topic mydemo1</span><br></pre></td></tr></table></figure></li>
<li>接收消息<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper spark81:2181 --topic mydemo1</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-搭建Spark-Streaming和Kafka的集成开发环境"><a href="#3-搭建Spark-Streaming和Kafka的集成开发环境" class="headerlink" title="3.搭建Spark Streaming和Kafka的集成开发环境"></a>3.搭建Spark Streaming和Kafka的集成开发环境</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;由于Spark Streaming和Kafka集成的时候，依赖的jar包比较多，而且还会产生冲突。强烈建议使用Maven的方式来搭建项目工程。<br>下面是依赖的pom.xml文件：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWY0NDNmNjA2Y2Y4ZjRkY2MucG5n?x-oss-process=image/format,png"></p>
<h4 id="4-基于Receiver的方式"><a href="#4-基于Receiver的方式" class="headerlink" title="4.基于Receiver的方式"></a>4.基于Receiver的方式</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;这个方法使用了Receivers来接收数据。Receivers的实现使用到Kafka高层次的消费者API。对于所有的Receivers，接收到的数据将会保存在Spark executors中，然后由Spark Streaming启动的Job来处理这些数据。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTVmZDc1ODhkZWMzYWQ2NjcucG5n?x-oss-process=image/format,png"></p>
<h5 id="1-开发Spark-Streaming的Kafka-Receivers"><a href="#1-开发Spark-Streaming的Kafka-Receivers" class="headerlink" title="(1)开发Spark Streaming的Kafka Receivers"></a>(1)开发Spark Streaming的Kafka Receivers</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQ2MDM1OWJmMTQ4YTM2NzkucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-测试"><a href="#2-测试" class="headerlink" title="(2)测试"></a>(2)测试</h5><ul>
<li>启动Kafka消息的生产者<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list spark81:9092 --topic mydemo1</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中启动任务，接收Kafka消息</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQ1NjgyNmU5MzY5MDgwNGEucG5n?x-oss-process=image/format,png"></p>
<h4 id="5-直接读取方式"><a href="#5-直接读取方式" class="headerlink" title="5.直接读取方式"></a>5.直接读取方式</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;和基于Receiver接收数据不一样，这种方式定期地从Kafka的topic+partition中查询最新的偏移量，再根据定义的偏移量范围在每个batch里面处理数据。当作业需要处理的数据来临时，spark通过调用Kafka的简单消费者API读取一定范围的数据。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE4ZDQ4YzI5ODY1MjczYzkucG5n?x-oss-process=image/format,png"></p>
<h5 id="1-开发Spark-Streaming的程序"><a href="#1-开发Spark-Streaming的程序" class="headerlink" title="(1)开发Spark Streaming的程序"></a>(1)开发Spark Streaming的程序</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTdmYjViNGQ2ZDQ3MGQ3MmEucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-测试-1"><a href="#2-测试-1" class="headerlink" title="(2)测试"></a>(2)测试</h5><ul>
<li>启动Kafka消息的生产者<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list spark81:9092 --topic mydemo1</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中启动任务，接收Kafka消息</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTEyMzUyZTYzYTRlY2UyZGMucG5n?x-oss-process=image/format,png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-Streaming/" rel="tag">Spark Streaming</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark Streaming：进阶"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E8%BF%9B%E9%98%B6/"
    >Spark Streaming：进阶</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E8%BF%9B%E9%98%B6/" class="article-date">
  <time datetime="2020-03-10T10:00:00.000Z" itemprop="datePublished">2020-03-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-StreamingContext对象详解"><a href="#一-StreamingContext对象详解" class="headerlink" title="一.StreamingContext对象详解"></a>一.StreamingContext对象详解</h3><h4 id="1-初始化StreamingContext"><a href="#1-初始化StreamingContext" class="headerlink" title="1.初始化StreamingContext"></a>1.初始化StreamingContext</h4><h5 id="1-方式一：从SparkConf对象中创建"><a href="#1-方式一：从SparkConf对象中创建" class="headerlink" title="(1)方式一：从SparkConf对象中创建"></a>(1)方式一：从SparkConf对象中创建</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;MyNetworkWordCount&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line"></span><br><span class="line">val src = <span class="keyword">new</span> StreamContext(conf, Second(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWRiYTQ3OGExMDI5OGQzNGQucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-方式二-从一个现有的SparkContext实例中创建"><a href="#2-方式二-从一个现有的SparkContext实例中创建" class="headerlink" title="(2)方式二:从一个现有的SparkContext实例中创建"></a>(2)方式二:从一个现有的SparkContext实例中创建</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala &gt;<span class="keyword">import</span> org.apache.spark.streaming.&#123;Second,StreamingContext&#125;</span><br><span class="line"></span><br><span class="line">scala &gt;val ssc=<span class="keyword">new</span> StreamContext(sc,Second(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTc5NmU2MDcwYzRkNjgyMzAucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-程序中的几点说明："><a href="#2-程序中的几点说明：" class="headerlink" title="2.程序中的几点说明："></a>2.程序中的几点说明：</h4><ul>
<li><p>appName参数是应用程序在集群UI上显示的名称。 </p>
</li>
<li><p>master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行。</p>
</li>
<li><p>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序(请确保本地系统中的cpu核心数够用) </p>
</li>
<li><p>StreamingContext会内在的创建一个SparkContext的实例(所有Spark功能的起始点)，你可以通过ssc.sparkContext访问到这个实例。</p>
</li>
<li><p>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。</p>
</li>
</ul>
<h4 id="3-请务必记住以下几点："><a href="#3-请务必记住以下几点：" class="headerlink" title="3.请务必记住以下几点："></a>3.请务必记住以下几点：</h4><ul>
<li><p>一旦一个StreamingContextt开始运作，就不能设置或添加新的流计算。</p>
</li>
<li><p>一旦一个上下文被停止，它将无法重新启动。</p>
</li>
<li><p>同一时刻，一个JVM中只能有一个StreamingContext处于活动状态。</p>
</li>
<li><p>StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext(保持SparkContext活跃)，请将stop() 方法的可选参数stopSparkContext设置为false。</p>
</li>
<li><p>只要前一个StreamingContext在下一个StreamingContext被创建之前停止(不停止SparkContext)，SparkContext就可以被重用来创建多个StreamingContext。</p>
</li>
</ul>
<h3 id="二-离散流-DStreams-：Discretized-Streams"><a href="#二-离散流-DStreams-：Discretized-Streams" class="headerlink" title="二.离散流(DStreams)：Discretized Streams"></a>二.离散流(DStreams)：Discretized Streams</h3><h4 id="1-DiscretizedStream或DStream-是Spark-Streaming对流式数据的基本抽象。"><a href="#1-DiscretizedStream或DStream-是Spark-Streaming对流式数据的基本抽象。" class="headerlink" title="1.DiscretizedStream或DStream 是Spark Streaming对流式数据的基本抽象。"></a>1.DiscretizedStream或DStream 是Spark Streaming对流式数据的基本抽象。</h4><p>它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，DStream由一系列连续的RDD表示，如下图：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWRkZTFhNTM3NmM1N2VjNzQucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-举例分析："><a href="#2-举例分析：" class="headerlink" title="2.举例分析："></a>2.举例分析：</h4><p>在之前的NetworkWordCount的例子中，我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWIzODg1YTcxNjBlMWM3YWIucG5n?x-oss-process=image/format,png"></p>
<p>但是DStream和RDD也有区别，下面画图说明：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWYyYTM4MjcyMjA5YWY2MWUucG5n?x-oss-process=image/format,png" alt="RDD的结构"></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE5Yjg5NDQ2OGIyMWRiNWYucG5n?x-oss-process=image/format,png" alt="DStream的结构"></p>
<h3 id="三-DStream中的转换操作-transformation"><a href="#三-DStream中的转换操作-transformation" class="headerlink" title="三.DStream中的转换操作(transformation)"></a>三.DStream中的转换操作(transformation)</h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTFmMzQxZDU2ODhhMjViMTAucG5n?x-oss-process=image/format,png"></p>
<p>最后两个transformation算子需要重点介绍一下：</p>
<h4 id="1-transform-func"><a href="#1-transform-func" class="headerlink" title="1.transform(func)"></a>1.transform(func)</h4><p>(1)通过RDD-to-RDD函数作用于源DStream中的各个RDD，可以是任意的RDD操作，从而返回一个新的RDD</p>
<p>(2)举例：在NetworkWordCount中，也可以使用transform来生成元组对</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWFhMTQ2MzU4YzQ1MTZkYWIucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-updateStateByKey-func"><a href="#2-updateStateByKey-func" class="headerlink" title="2.updateStateByKey(func)"></a>2.updateStateByKey(func)</h4><h5 id="1-操作允许不断用新信息更新它的同时保持任意状态。"><a href="#1-操作允许不断用新信息更新它的同时保持任意状态。" class="headerlink" title="(1)操作允许不断用新信息更新它的同时保持任意状态。"></a>(1)操作允许不断用新信息更新它的同时保持任意状态。</h5><ul>
<li>定义状态-状态可以是任何的数据类型</li>
<li>定义状态更新函数-怎样利用更新前的状态和从输入流里面获取的新值更新状态</li>
</ul>
<h5 id="2-重写NetworkWordCount程序，累计每个单词出现的频率-注意：累计"><a href="#2-重写NetworkWordCount程序，累计每个单词出现的频率-注意：累计" class="headerlink" title="(2)重写NetworkWordCount程序，累计每个单词出现的频率(注意：累计)"></a>(2)重写NetworkWordCount程序，累计每个单词出现的频率(注意：累计)</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTg0MzAxMDYyYjIxODBkYWEucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-输出结果："><a href="#3-输出结果：" class="headerlink" title="(3)输出结果："></a>(3)输出结果：</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWJkMDA2OGFkYzNhODAwNzMucG5n?x-oss-process=image/format,png"></p>
<h4 id="3-注意："><a href="#3-注意：" class="headerlink" title="3.注意："></a>3.注意：</h4><p>如果在IDEA中，不想输出log4j的日志信息，可以将log4j.properties文件(放在src的目录下)的第一行改为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootCategory=ERROR,console</span><br></pre></td></tr></table></figure>

<h3 id="四-窗口操作"><a href="#四-窗口操作" class="headerlink" title="四.窗口操作"></a>四.窗口操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming还提供了窗口计算功能，允许您在数据的滑动窗口上应用转换操作。下图说明了滑动窗口的工作方式：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTNmYTEzZWJiMWQ5ZDRlMjcucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;如图所示，每当窗口滑过originalDStream时，落在窗口内的源RDD被组合并被执行操作以产生windowed DStream的RDD。在上面的例子中，操作应用于最近3个时间单位的数据，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数。</p>
<ul>
<li>窗口长度(windowlength) - 窗口的时间长度(上图的示例中为：3)。</li>
<li>滑动间隔(slidinginterval) - 两次相邻的窗口操作的间隔(即每次滑动的时间长度)(上图示例中为：2)。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;这两个参数必须是源DStream的批间隔的倍数(上图示例中为：1)。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;我们以一个例子来说明窗口操作。 假设您希望对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的pairs DStream数据中对(word, 1)键值对应用reduceByKey操作。这是通过使用reduceByKeyAndWindow操作完成的。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTQxNGI2YzllYWY4OWZmOGMucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数 - windowLength和slideInterval。</p>
<h5 id="1-window-windowLength-slideInterval"><a href="#1-window-windowLength-slideInterval" class="headerlink" title="(1)window(windowLength, slideInterval)"></a>(1)window(windowLength, slideInterval)</h5><ul>
<li>基于源DStream产生的窗口化的批数据计算一个新的DStream</li>
</ul>
<h5 id="2-countByWindow-windowLength-slideInterval"><a href="#2-countByWindow-windowLength-slideInterval" class="headerlink" title="(2)countByWindow(windowLength, slideInterval)"></a>(2)countByWindow(windowLength, slideInterval)</h5><ul>
<li>返回流中元素的一个滑动窗口数</li>
</ul>
<h5 id="3-reduceByWindow-func-windowLength-slideInterval"><a href="#3-reduceByWindow-func-windowLength-slideInterval" class="headerlink" title="(3)reduceByWindow(func, windowLength, slideInterval)"></a>(3)reduceByWindow(func, windowLength, slideInterval)</h5><ul>
<li>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</li>
</ul>
<h5 id="4-reduceByKeyAndWindow-func-windowLength-slideInterval-numTasks"><a href="#4-reduceByKeyAndWindow-func-windowLength-slideInterval-numTasks" class="headerlink" title="(4)reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])"></a>(4)reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</h5><ul>
<li>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</li>
</ul>
<h5 id="5-reduceByKeyAndWindow-func-invFunc-windowLength-slideInterval-numTasks"><a href="#5-reduceByKeyAndWindow-func-invFunc-windowLength-slideInterval-numTasks" class="headerlink" title="(5)reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])"></a>(5)reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</h5><ul>
<li>上述reduceByKeyAndWindow() 的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减(inverse reducing)”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数(invertible reduce functions)”，即具有相应“反减”功能的减函数(作为参数invFunc)。 像reduceByKeyAndWindow一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</li>
</ul>
<h5 id="6-countByValueAndWindow-windowLength-slideInterval-numTasks"><a href="#6-countByValueAndWindow-windowLength-slideInterval-numTasks" class="headerlink" title="(6)countByValueAndWindow(windowLength, slideInterval, [numTasks])"></a>(6)countByValueAndWindow(windowLength, slideInterval, [numTasks])</h5><ul>
<li>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</li>
</ul>
<h3 id="五-输入DStreams和接收器"><a href="#五-输入DStreams和接收器" class="headerlink" title="五.输入DStreams和接收器"></a>五.输入DStreams和接收器</h3><h4 id="1-输入DStreams表示从数据源获取输入数据流的DStreams。"><a href="#1-输入DStreams表示从数据源获取输入数据流的DStreams。" class="headerlink" title="1.输入DStreams表示从数据源获取输入数据流的DStreams。"></a>1.输入DStreams表示从数据源获取输入数据流的DStreams。</h4><p>在NetworkWordCount例子中，lines表示输入DStream，它代表从netcat服务器获取的数据流。每一个输入流DStream和一个Receiver对象相关联，这个Receiver从源中获取数据，并将数据存入内存中用于处理。<br><strong>输入DStreams表示从数据源获取的原始数据流</strong></p>
<h4 id="2-Spark-Streaming拥有两类数据源："><a href="#2-Spark-Streaming拥有两类数据源：" class="headerlink" title="2.Spark Streaming拥有两类数据源："></a>2.Spark Streaming拥有两类数据源：</h4><h5 id="1-基本源-Basic-sources-：这些源在StreamingContext-API中直接可用。例如文件系统、套接字连接、Akka的actor等"><a href="#1-基本源-Basic-sources-：这些源在StreamingContext-API中直接可用。例如文件系统、套接字连接、Akka的actor等" class="headerlink" title="(1)基本源(Basic sources)：这些源在StreamingContext API中直接可用。例如文件系统、套接字连接、Akka的actor等"></a>(1)基本源(Basic sources)：这些源在StreamingContext API中直接可用。例如文件系统、套接字连接、Akka的actor等</h5><h5 id="2-高级源-Advanced-sources-：这些源包括Kafka-Flume-Kinesis-Twitter等等。"><a href="#2-高级源-Advanced-sources-：这些源包括Kafka-Flume-Kinesis-Twitter等等。" class="headerlink" title="(2)高级源(Advanced sources)：这些源包括Kafka,Flume,Kinesis,Twitter等等。"></a>(2)高级源(Advanced sources)：这些源包括Kafka,Flume,Kinesis,Twitter等等。</h5><h4 id="3-下面通过具体的案例，详细说明："><a href="#3-下面通过具体的案例，详细说明：" class="headerlink" title="3.下面通过具体的案例，详细说明："></a>3.下面通过具体的案例，详细说明：</h4><h5 id="1-文件流：通过监控文件系统的变化，若有新文件添加，则将它读入并作为数据流"><a href="#1-文件流：通过监控文件系统的变化，若有新文件添加，则将它读入并作为数据流" class="headerlink" title="(1)文件流：通过监控文件系统的变化，若有新文件添加，则将它读入并作为数据流"></a>(1)文件流：通过监控文件系统的变化，若有新文件添加，则将它读入并作为数据流</h5><p>需要注意的是：</p>
<ul>
<li>这些文件具有相同的格式</li>
<li>这些文件通过原子移动或重命名文件的方式在dataDirectory创建</li>
<li>如果在文件中追加内容，这些追加的新数据也不会被读取。</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTE2NzIwMWM2MTdlNmY2ZmUucG5n?x-oss-process=image/format,png"></p>
<p>注意：要演示成功，需要在原文件中编辑，然后拷贝一份。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBhMWI0NTI0NjI5N2YzYjMucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-RDD队列流"><a href="#2-RDD队列流" class="headerlink" title="(2)RDD队列流"></a>(2)RDD队列流</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;使用streamingContext.queueStream(queueOfRDD)创建基于RDD队列的DStream，用于调试Spark Streaming应用程序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWMyY2ZhMWM5MTZhYjMzMTUucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-套接字流：通过监听Socket端口来接收数据"><a href="#3-套接字流：通过监听Socket端口来接收数据" class="headerlink" title="(3)套接字流：通过监听Socket端口来接收数据"></a>(3)套接字流：通过监听Socket端口来接收数据</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQ4NzY3OTRhOWE3NTdkZDcucG5n?x-oss-process=image/format,png"></p>
<h3 id="六-DStreams的输出操作"><a href="#六-DStreams的输出操作" class="headerlink" title="六.DStreams的输出操作"></a>六.DStreams的输出操作</h3><h4 id="1-输出操作允许DStream的操作推到如数据库、文件系统等外部系统中。"><a href="#1-输出操作允许DStream的操作推到如数据库、文件系统等外部系统中。" class="headerlink" title="1.输出操作允许DStream的操作推到如数据库、文件系统等外部系统中。"></a>1.输出操作允许DStream的操作推到如数据库、文件系统等外部系统中。</h4><p>因为输出操作实际上是允许外部系统消费转换后的数据，它们触发的实际操作是DStream转换。</p>
<h4 id="2-目前，定义了下面几种输出操作："><a href="#2-目前，定义了下面几种输出操作：" class="headerlink" title="2.目前，定义了下面几种输出操作："></a>2.目前，定义了下面几种输出操作：</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTg4NzBlZWUxNzk2NjI2NmEucG5n?x-oss-process=image/format,png"></p>
<h5 id="1-foreachRDD的设计模式"><a href="#1-foreachRDD的设计模式" class="headerlink" title="(1)foreachRDD的设计模式"></a>(1)foreachRDD的设计模式</h5><p>DStream.foreachRDD是一个强大的原语，发送数据到外部系统中。</p>
<h6 id="a-第一步：创建连接，将数据写入外部数据库-使用之前的NetworkWordCount，改写之前输出结果的部分，如下"><a href="#a-第一步：创建连接，将数据写入外部数据库-使用之前的NetworkWordCount，改写之前输出结果的部分，如下" class="headerlink" title="(a)第一步：创建连接，将数据写入外部数据库(使用之前的NetworkWordCount，改写之前输出结果的部分，如下)"></a>(a)第一步：创建连接，将数据写入外部数据库(使用之前的NetworkWordCount，改写之前输出结果的部分，如下)</h6><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWM3MTAzNDAxNDI4ZjkyNzkucG5n?x-oss-process=image/format,png"></p>
<p>出现以下Exception：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRkMmUwOTJhZTM0MWQ2ZmYucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;原因是：Connection对象不是一个可被序列化的对象，不能RDD的每个Worker上运行；即：Connection不能在RDD分布式环境中的每个分区上运行，因为不同的分区可能运行在不同的Worker上。所以需要在每个RDD分区上单独创建Connection对象。</p>
<h6 id="b-第二步：在每个RDD分区上单独创建Connection对象，如下："><a href="#b-第二步：在每个RDD分区上单独创建Connection对象，如下：" class="headerlink" title="(b)第二步：在每个RDD分区上单独创建Connection对象，如下："></a>(b)第二步：在每个RDD分区上单独创建Connection对象，如下：</h6><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRiZjQxMGExOTk0OTgwMTkucG5n?x-oss-process=image/format,png"></p>
<h3 id="七-DataFrame和SQL操作"><a href="#七-DataFrame和SQL操作" class="headerlink" title="七.DataFrame和SQL操作"></a>七.DataFrame和SQL操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;可以很方便地使用DataFrames和SQL操作来处理流数据。必须使用当前的StreamingContext对应的SparkContext创建一个SparkSession。此外，必须这样做的另一个原因是使得应用可以在driver程序故障时得以重新启动，这是通过创建一个可以延迟实例化的单例SparkSession来实现的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;在下面的示例中，使用DataFrames和SQL来修改之前的wordcount示例并对单词进行计数。我们将每个RDD转换为DataFrame，并注册为临时表，然后在这张表上执行SQL查询。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQ3ZDBjMDcwY2QwZjVkYjAucG5n?x-oss-process=image/format,png"></p>
<h3 id="八-缓存-持久化"><a href="#八-缓存-持久化" class="headerlink" title="八.缓存/持久化"></a>八.缓存/持久化</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;与RDD类似，DStreams还允许开发人员将流数据保留在内存中。也就是说，在DStream上调用persist() 方法会自动将该DStream的每个RDD保留在内存中。如果DStream中的数据将被多次计算(例如，相同数据上执行多个操作)，这个操作就会很有用。对于基于窗口的操作，如reduceByWindow和reduceByKeyAndWindow以及基于状态的操作，如updateStateByKey，数据会默认进行持久化。 因此，基于窗口的操作生成的DStream会自动保存在内存中，而不需要开发人员调用persist()。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;对于通过网络接收数据（例如Kafka，Flume，sockets等）的输入流，默认持久化级别被设置为将数据复制到两个节点进行容错。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;请注意，与RDD不同，DStreams的默认持久化级别将数据序列化保存在内存中。</p>
<h3 id="九-检查点支持"><a href="#九-检查点支持" class="headerlink" title="九.检查点支持"></a>九.检查点支持</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;流数据处理程序通常都是全天候运行，因此必须对应用中逻辑无关的故障(例如，系统故障，JVM崩溃等)具有弹性。为了实现这一特性，Spark Streaming需要checkpoint足够的信息到容错存储系统，以便可以从故障中恢复。</p>
<h4 id="1-一般会对两种类型的数据使用检查点："><a href="#1-一般会对两种类型的数据使用检查点：" class="headerlink" title="1.一般会对两种类型的数据使用检查点："></a>1.一般会对两种类型的数据使用检查点：</h4><h5 id="1-元数据检查点-Metadatacheckpointing"><a href="#1-元数据检查点-Metadatacheckpointing" class="headerlink" title="(1)元数据检查点(Metadatacheckpointing)"></a>(1)元数据检查点(Metadatacheckpointing)</h5><p>将定义流计算的信息保存到容错存储中（如HDFS）。这用于从运行streaming程序的driver程序的节点的故障中恢复。元数据包括以下几种：</p>
<ul>
<li>配置（Configuration） - 用于创建streaming应用程序的配置信息。</li>
<li>DStream操作（DStream operations） - 定义streaming应用程序的DStream操作集合。</li>
<li>不完整的batch（Incomplete batches） - jobs还在队列中但尚未完成的batch。</li>
</ul>
<h5 id="2-数据检查点-Datacheckpointing"><a href="#2-数据检查点-Datacheckpointing" class="headerlink" title="(2)数据检查点(Datacheckpointing)"></a>(2)数据检查点(Datacheckpointing)</h5><p>将生成的RDD保存到可靠的存储层。对于一些需要将多个批次之间的数据进行组合的stateful变换操作，设置数据检查点是必需的。在这些转换操作中，当前生成的RDD依赖于先前批次的RDD，这导致依赖链的长度随时间而不断增加，由此也会导致基于血统机制的恢复时间无限增加。为了避免这种情况，stateful转换的中间RDD将定期设置检查点并保存到到可靠的存储层（例如HDFS）以切断依赖关系链。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;总而言之，元数据检查点主要用于从driver程序故障中恢复，而数据或RDD检查点在任何使用stateful转换时是必须要有的。</p>
<h4 id="2-何时启用检查点："><a href="#2-何时启用检查点：" class="headerlink" title="2.何时启用检查点："></a>2.何时启用检查点：</h4><p>对于具有以下任一要求的应用程序，必须启用检查点：</p>
<h5 id="1-使用状态转："><a href="#1-使用状态转：" class="headerlink" title="(1)使用状态转："></a>(1)使用状态转：</h5><p>如果在应用程序中使用updateStateByKey或reduceByKeyAndWindow(具有逆函数)，则必须提供检查点目录以允许定期保存RDD检查点。</p>
<h5 id="2-从运行应用程序的driver程序的故障中恢复："><a href="#2-从运行应用程序的driver程序的故障中恢复：" class="headerlink" title="(2)从运行应用程序的driver程序的故障中恢复："></a>(2)从运行应用程序的driver程序的故障中恢复：</h5><p>元数据检查点用于使用进度信息进行恢复。</p>
<h4 id="3-如何配置检查点："><a href="#3-如何配置检查点：" class="headerlink" title="3.如何配置检查点："></a>3.如何配置检查点：</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;可以通过在一些可容错、高可靠的文件系统（例如，HDFS，S3等）中设置保存检查点信息的目录来启用检查点。这是通过使用streamingContext.checkpoint(checkpointDirectory)完成的。设置检查点后，您就可以使用上述的有状态转换操作。此外，如果要使应用程序从驱动程序故障中恢复，您应该重写streaming应用程序以使程序具有以下行为：</p>
<p>(a)当程序第一次启动时，它将创建一个新的StreamingContext，设置好所有流数据源，然后调用start()方法。</p>
<p>(b)当程序在失败后重新启动时，它将从checkpoint目录中的检查点数据重新创建一个StreamingContext。<br>使用StreamingContext.getOrCreate可以简化此行为</p>
<h4 id="4-改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下"><a href="#4-改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下" class="headerlink" title="4.改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下"></a>4.改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下</h4><p>通过查看HDFS中的信息，可以看到相关的检查点信息，如下：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTUxNzI2M2VmMzg1NGFjZTEucG5n?x-oss-process=image/format,png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-Streaming/" rel="tag">Spark Streaming</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark Streaming：基础"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E5%9F%BA%E7%A1%80/"
    >Spark Streaming：基础</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/10/Spark%20Streaming%EF%BC%9A%E5%9F%BA%E7%A1%80/" class="article-date">
  <time datetime="2020-03-10T09:00:00.000Z" itemprop="datePublished">2020-03-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-Spark-Streaming简介"><a href="#1-Spark-Streaming简介" class="headerlink" title="1.Spark Streaming简介"></a>1.Spark Streaming简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数(如map，reduce，join和window)开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且，您还可以在数据流上应用Spark提供的机器学习和图处理算法。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWI5MjhiMjJjMGMwYTE1ZjEucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-Spark-Streaming的特点"><a href="#2-Spark-Streaming的特点" class="headerlink" title="2.Spark Streaming的特点"></a>2.Spark Streaming的特点</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWMwOTgzOTE1MTg1NGY5YTQucG5n?x-oss-process=image/format,png"></p>
<h5 id="1-易用：集成在Spark中"><a href="#1-易用：集成在Spark中" class="headerlink" title="(1)易用：集成在Spark中"></a>(1)易用：集成在Spark中</h5><h5 id="2-容错性：底层RDD，RDD本身就具备容错机制。"><a href="#2-容错性：底层RDD，RDD本身就具备容错机制。" class="headerlink" title="(2)容错性：底层RDD，RDD本身就具备容错机制。"></a>(2)容错性：底层RDD，RDD本身就具备容错机制。</h5><h5 id="3-支持多种编程语言：Java-Scala-Python"><a href="#3-支持多种编程语言：Java-Scala-Python" class="headerlink" title="(3)支持多种编程语言：Java Scala Python"></a>(3)支持多种编程语言：Java Scala Python</h5><h4 id="3-Spark-Streaming的内部结构"><a href="#3-Spark-Streaming的内部结构" class="headerlink" title="3.Spark Streaming的内部结构"></a>3.Spark Streaming的内部结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在内部，它的工作原理如下。Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWI3MTAxMmJmYjMwZDMyMjUucG5n?x-oss-process=image/format,png"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming将连续的数据流抽象为discretizedstream或DStream。在内部，DStream 由一个RDD序列表示。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark-Streaming/" rel="tag">Spark Streaming</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark Core：Spark RDD的高级算子"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/10/Spark%20Core%EF%BC%9ASpark%20RDD%E7%9A%84%E9%AB%98%E7%BA%A7%E7%AE%97%E5%AD%90/"
    >Spark Core：Spark RDD的高级算子</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2020/03/10/Spark%20Core%EF%BC%9ASpark%20RDD%E7%9A%84%E9%AB%98%E7%BA%A7%E7%AE%97%E5%AD%90/" class="article-date">
  <time datetime="2020-03-10T08:00:00.000Z" itemprop="datePublished">2020-03-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-mapPartitionsWithIndex"><a href="#1-mapPartitionsWithIndex" class="headerlink" title="1.mapPartitionsWithIndex"></a>1.mapPartitionsWithIndex</h4><p>把每个partition中的分区号和对应的值拿出来</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) ⇒ Iterator[U])</span><br></pre></td></tr></table></figure>

<h5 id="1-参数说明："><a href="#1-参数说明：" class="headerlink" title="(1)参数说明："></a>(1)参数说明：</h5><ul>
<li>f是一个函数参数，需要自定义。</li>
<li>f 接收两个参数，第一个参数是Int，代表分区号。第二个Iterator[T]代表分区中的元素。</li>
</ul>
<h5 id="2-通过这两个参数，可以定义处理分区的函数。"><a href="#2-通过这两个参数，可以定义处理分区的函数。" class="headerlink" title="(2)通过这两个参数，可以定义处理分区的函数。"></a>(2)通过这两个参数，可以定义处理分区的函数。</h5><p>Iterator[U] ： 操作完成后，返回的结果。</p>
<h5 id="3-示例：将每个分区中的元素和分区号打印出来。"><a href="#3-示例：将每个分区中的元素和分区号打印出来。" class="headerlink" title="(3)示例：将每个分区中的元素和分区号打印出来。"></a>(3)示例：将每个分区中的元素和分区号打印出来。</h5><h6 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h6 id="b-创建一个函数返回RDD中的每个分区号和元素："><a href="#b-创建一个函数返回RDD中的每个分区号和元素：" class="headerlink" title="(b)创建一个函数返回RDD中的每个分区号和元素："></a>(b)创建一个函数返回RDD中的每个分区号和元素：</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">func1</span><span class="params">(index:Int, iter:Iterator[Int])</span>:Iterator[String] </span>=&#123;</span><br><span class="line">   iter.toList.map( x =&gt; <span class="string">&quot;[PartID:&quot;</span> + index + <span class="string">&quot;, value=&quot;</span> + x + <span class="string">&quot;]&quot;</span> ).iterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="c-调用："><a href="#c-调用：" class="headerlink" title="(c )调用："></a>(c )调用：</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.mapPartitionsWithIndex(func1).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTYzNzJjZjQzYzViY2M1NjYucG5n?x-oss-process=image/format,png"></p>
<h4 id="2-aggregate"><a href="#2-aggregate" class="headerlink" title="2.aggregate"></a>2.aggregate</h4><p><strong>含义</strong>：先对局部聚合，再对全局聚合</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTEyZmE5ZTBhNjViODQ1MDQucG5n?x-oss-process=image/format,png"></p>
<p><strong>举例</strong>：</p>
<h5 id="1-第一个例子："><a href="#1-第一个例子：" class="headerlink" title="(1)第一个例子："></a>(1)第一个例子：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">func1</span><span class="params">(index:Int, iter:Iterator[Int])</span>:Iterator[String] </span>=&#123;</span><br><span class="line">   iter.toList.map( x =&gt; <span class="string">&quot;[PartID:&quot;</span> + index + <span class="string">&quot;, value=&quot;</span> + x + <span class="string">&quot;]&quot;</span> ).iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查看每个分区中的元素：</span></span><br><span class="line">rdd1.mapPartitionsWithIndex(func1).collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWQ2YmJjZTg1ZjZkMjUxNDUucG5n?x-oss-process=image/format,png"></p>
<h6 id="a-需求：将每个分区中的最大值求和，注意：初始值是0；"><a href="#a-需求：将每个分区中的最大值求和，注意：初始值是0；" class="headerlink" title="(a)需求：将每个分区中的最大值求和，注意：初始值是0；"></a>(a)需求：将每个分区中的最大值求和，注意：初始值是0；</h6><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWZhZDk0YmZjNjM2MTY3NGUucG5n?x-oss-process=image/format,png" alt="分析过程"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果初始值为0，结果为7</span></span><br><span class="line">rdd1.aggregate(<span class="number">0</span>)(max(_,_),_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//如果初始值为10，则结果为：30</span></span><br><span class="line">rdd1.aggregate(<span class="number">10</span>)(max(_,_),_+_)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTFhMGQxNzMzMzA4ZDUwYWQucG5n?x-oss-process=image/format,png"></p>
<h6 id="b-需求：如果是求和，注意：初始值是0："><a href="#b-需求：如果是求和，注意：初始值是0：" class="headerlink" title="(b)需求：如果是求和，注意：初始值是0："></a>(b)需求：如果是求和，注意：初始值是0：</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果初始值是0</span></span><br><span class="line">rdd1.aggregate(<span class="number">0</span>)(_+_,_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//如果初始值是10，则结果是：45</span></span><br><span class="line">rdd1.aggregate(<span class="number">10</span>)(_+_,_+_)</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWZjNmYzZTA0Y2NjODBlNTQucG5n?x-oss-process=image/format,png"></p>
<h5 id="2-第二个例子：一个字符串的例子："><a href="#2-第二个例子：一个字符串的例子：" class="headerlink" title="(2)第二个例子：一个字符串的例子："></a>(2)第二个例子：一个字符串的例子：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = sc.parallelize(List(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>,<span class="string">&quot;d&quot;</span>,<span class="string">&quot;e&quot;</span>,<span class="string">&quot;f&quot;</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//修改一下刚才的查看分区元素的函数</span></span><br><span class="line"><span class="function">def <span class="title">func2</span><span class="params">(index: Int, iter: Iterator[(String)</span>]) : Iterator[String] </span>= &#123;</span><br><span class="line">  iter.toList.map(x =&gt; <span class="string">&quot;[partID:&quot;</span> +  index + <span class="string">&quot;, val: &quot;</span> + x + <span class="string">&quot;]&quot;</span>).iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查看两个分区中的元素：</span></span><br><span class="line">rdd2.mapPartitionsWithIndex(func2).collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWU3Y2RhNjA2ZDcxMjNlMWMucG5n?x-oss-process=image/format,png"></p>
<p>运行结果：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd2.aggregate(<span class="string">&quot;&quot;</span>)(_+_,_+_)</span><br><span class="line"></span><br><span class="line">rdd2.aggregate(<span class="string">&quot;*&quot;</span>)(_+_,_+_)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWFhMmJhMmEwNDI5OGQyZjAucG5n?x-oss-process=image/format,png" alt="结果"></p>
<h5 id="3-例三：更复杂一点的例子"><a href="#3-例三：更复杂一点的例子" class="headerlink" title="(3)例三：更复杂一点的例子"></a>(3)例三：更复杂一点的例子</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = sc.parallelize(List(<span class="string">&quot;12&quot;</span>,<span class="string">&quot;23&quot;</span>,<span class="string">&quot;345&quot;</span>,<span class="string">&quot;4567&quot;</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd3.mapPartitionsWithIndex(func2).collect</span><br><span class="line"></span><br><span class="line">rdd3.aggregate(<span class="string">&quot;&quot;</span>)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTZlZDJiNGJiODE1N2I2YzEucG5n?x-oss-process=image/format,png" alt="结果"></p>
<p>程序执行分析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一个分区： &quot;12&quot;  &quot;23&quot;</span><br><span class="line">    第一次比较：&quot;&quot; 和 &quot;12&quot; 比，求长度的最大值: 2 。 2 ---&gt; &quot;2&quot;</span><br><span class="line">    第二次比较：&quot;2&quot; 和 &quot;23&quot; 比，求长度的最大值: 2。 2 ---&gt; &quot;2&quot;</span><br><span class="line">				</span><br><span class="line">第二个分区：&quot;345&quot;  &quot;4567&quot;</span><br><span class="line">    第一次比较：&quot;&quot; 和 &quot;345&quot; 比，求长度的最大值: 3 。 3 ---&gt; &quot;3&quot;</span><br><span class="line">    第二次比较：&quot;3&quot; 和 &quot;4567&quot; 比，求长度的最大值: 4。 4 ---&gt; &quot;4&quot;</span><br></pre></td></tr></table></figure>
<p>结果可能是：”24”，也可能是：”42”</p>
<h5 id="4-例四："><a href="#4-例四：" class="headerlink" title="(4)例四："></a>(4)例四：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd4 = sc.parallelize(List(<span class="string">&quot;12&quot;</span>,<span class="string">&quot;23&quot;</span>,<span class="string">&quot;345&quot;</span>,<span class="string">&quot;&quot;</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd4.mapPartitionsWithIndex(func2).collect</span><br><span class="line"></span><br><span class="line">rdd4.aggregate(<span class="string">&quot;&quot;</span>)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWYwOTY4MDVjMTg1OWQyZGUucG5n?x-oss-process=image/format,png" alt="结果"></p>
<p>程序执行分析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一个分区： &quot;12&quot;  &quot;23&quot;</span><br><span class="line">    第一次比较：&quot;&quot; 和 &quot;12&quot; 比长度，求长度的最小值。 0 。 0 ---&gt; &quot;0&quot;</span><br><span class="line">    第二次比较：&quot;0&quot; 和 &quot;23&quot; 比长度，求长度的最小值。 1。 1 ---&gt; &quot;1&quot;</span><br><span class="line">				</span><br><span class="line">第二个分区：&quot;345&quot;  &quot;&quot;</span><br><span class="line">    第一次比较：&quot;&quot; 和 &quot;345&quot; 比，求长度的最小值。 0 。 0 ---&gt; &quot;0&quot;</span><br><span class="line">    第二次比较：&quot;0&quot; 和 &quot;&quot; 比，求长度的最小值。 0。 0 ---&gt; &quot;0&quot;</span><br></pre></td></tr></table></figure>
<p>结果是：”10”，也可能是”01”，</p>
<p>原因：注意有个初始值””，其长度0，然后0.toString变成字符串</p>
<h5 id="5-例5"><a href="#5-例5" class="headerlink" title="(5)例5:"></a>(5)例5:</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd5 = sc.parallelize(List(<span class="string">&quot;12&quot;</span>,<span class="string">&quot;23&quot;</span>,<span class="string">&quot;&quot;</span>,<span class="string">&quot;345&quot;</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd5.mapPartitionsWithIndex(func2).collect</span><br><span class="line"></span><br><span class="line">rdd5.aggregate(<span class="string">&quot;&quot;</span>)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWMxYmJmMWQ1OWYwMWVjY2MucG5n?x-oss-process=image/format,png" alt="结果"></p>
<p>结果是：”11”，原因同上</p>
<p>程序执行分析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一个分区： &quot;12&quot;  &quot;23&quot;</span><br><span class="line">		第一次比较：&quot;&quot; 和 &quot;12&quot; 比长度，求长度的最小值。 0 。 0 ---&gt; &quot;0&quot;</span><br><span class="line">		第二次比较：&quot;0&quot; 和 &quot;23&quot; 比长度，求长度的最小值。 1。 1 ---&gt; &quot;1&quot;</span><br><span class="line">				</span><br><span class="line">第二个分区：&quot;&quot;  &quot;345&quot;</span><br><span class="line">		第一次比较：&quot;&quot; 和 &quot;&quot; 比，求长度的最小值。 0 。 0 ---&gt; &quot;0&quot;</span><br><span class="line">		第二次比较：&quot;0&quot; 和 &quot;345&quot; 比，求长度的最小值。 1。 1 ---&gt; &quot;1&quot;	</span><br></pre></td></tr></table></figure>
<h4 id="3-aggregateByKey-类似于aggregate操作，区别：操作的-的数据"><a href="#3-aggregateByKey-类似于aggregate操作，区别：操作的-的数据" class="headerlink" title="3.aggregateByKey:类似于aggregate操作，区别：操作的  的数据"></a>3.aggregateByKey:类似于aggregate操作，区别：操作的 <key value> 的数据</h4><h5 id="1-准备数据："><a href="#1-准备数据：" class="headerlink" title="(1)准备数据："></a>(1)准备数据：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val pairRDD = sc.parallelize(List( (<span class="string">&quot;cat&quot;</span>,<span class="number">2</span>), (<span class="string">&quot;cat&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;mouse&quot;</span>, <span class="number">4</span>),(<span class="string">&quot;cat&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;dog&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;mouse&quot;</span>, <span class="number">2</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">func3</span><span class="params">(index: Int, iter: Iterator[(String, Int)</span>]) : Iterator[String] </span>= &#123;</span><br><span class="line">  iter.toList.map(x =&gt; <span class="string">&quot;[partID:&quot;</span> +  index + <span class="string">&quot;, val: &quot;</span> + x + <span class="string">&quot;]&quot;</span>).iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pairRDD.mapPartitionsWithIndex(func3).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBkYTYxNmIwOTZkZThmZTEucG5n?x-oss-process=image/format,png" alt="准备数据"></p>
<h5 id="2-两个分区中的元素："><a href="#2-两个分区中的元素：" class="headerlink" title="(2)两个分区中的元素："></a>(2)两个分区中的元素：</h5><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTg0ODFhZjc0ZDM2YmVjM2YucG5n?x-oss-process=image/format,png"></p>
<h5 id="3-示例："><a href="#3-示例：" class="headerlink" title="(3) 示例："></a>(3) 示例：</h5><h6 id="a-将每个分区中的动物最多的个数求和"><a href="#a-将每个分区中的动物最多的个数求和" class="headerlink" title="(a)将每个分区中的动物最多的个数求和"></a>(a)将每个分区中的动物最多的个数求和</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairRDD.aggregateByKey(<span class="number">0</span>)(math.max(_, _), _ + _).collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTUxNjVhMmZjMDI3NDBmMGIucG5n?x-oss-process=image/format,png" alt="结果"></p>
<h6 id="b-将每种动物个数求和"><a href="#b-将每种动物个数求和" class="headerlink" title="(b)将每种动物个数求和"></a>(b)将每种动物个数求和</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairRDD.aggregateByKey(<span class="number">0</span>)(_+_, _ + _).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTNmOTkyYmQ5NmY2NDkwZmMucG5n?x-oss-process=image/format,png" alt="结果"></p>
<h6 id="c-这个例子也可以使用：reduceByKey"><a href="#c-这个例子也可以使用：reduceByKey" class="headerlink" title="(c )这个例子也可以使用：reduceByKey"></a>(c )这个例子也可以使用：reduceByKey</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairRDD.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTBkMGJjY2MyNDY0M2YxOTQucG5n?x-oss-process=image/format,png" alt="结果"></p>
<p><strong>与reduceByKey相比，aggregateByKey 效率更高</strong></p>
<h4 id="4-coalesce与repartition"><a href="#4-coalesce与repartition" class="headerlink" title="4.coalesce与repartition"></a>4.coalesce与repartition</h4><h5 id="1-都是将RDD中的分区进行重分区。"><a href="#1-都是将RDD中的分区进行重分区。" class="headerlink" title="(1)都是将RDD中的分区进行重分区。"></a>(1)都是将RDD中的分区进行重分区。</h5><h5 id="2-区别是：coalesce默认不会进行shuffle-false-；而repartition会进行shuffle-true-，即：会将数据真正通过网络进行重分区。"><a href="#2-区别是：coalesce默认不会进行shuffle-false-；而repartition会进行shuffle-true-，即：会将数据真正通过网络进行重分区。" class="headerlink" title="(2)区别是：coalesce默认不会进行shuffle(false)；而repartition会进行shuffle(true)，即：会将数据真正通过网络进行重分区。"></a>(2)区别是：coalesce默认不会进行shuffle(false)；而repartition会进行shuffle(true)，即：会将数据真正通过网络进行重分区。</h5><h5 id="3-示例：-1"><a href="#3-示例：-1" class="headerlink" title="(3)示例："></a>(3)示例：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">func4</span><span class="params">(index: Int, iter: Iterator[(Int)</span>]) : Iterator[String] </span>= &#123;</span><br><span class="line">  iter.toList.map(x =&gt; <span class="string">&quot;[partID:&quot;</span> +  index + <span class="string">&quot;, val: &quot;</span> + x + <span class="string">&quot;]&quot;</span>).iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val rdd1 = sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd1.mapPartitionsWithIndex(func4).collect</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTcxZGYxMjIzY2M4NjM4NWQucG5n?x-oss-process=image/format,png" alt="数据准备"></p>
<p>下面两句话是等价的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = rdd1.repartition(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">val rdd3 = rdd1.coalesce(<span class="number">3</span>,<span class="keyword">true</span>)    <span class="comment">//---&gt;如果是false，查看RDD的length依然是2</span></span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWEwOTk1MDZjNTFmZDI0YmQucG5n?x-oss-process=image/format,png" alt="结果"></p>
<h4 id="5、其他高级算子"><a href="#5、其他高级算子" class="headerlink" title="5、其他高级算子"></a>5、其他高级算子</h4><p>参考：<a target="_blank" rel="noopener" href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html">http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SparkCore/" rel="tag">SparkCore</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/8/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/10/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> Movle
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="https://img-blog.csdnimg.cn/20200609161448519.jpg" alt="Movle"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>