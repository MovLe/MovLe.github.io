<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Movle</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="https://img-blog.csdnimg.cn/20200609161448519.jpg" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="https://img-blog.csdnimg.cn/2020060916514052.png" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Movle</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['种一棵树，最好的时机是十年前，其次是现在', '人必有痴，而后有成', '今天，我没有浑浑噩噩的度过'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-HDFS-HA高可用集群配置"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/02/HDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE/"
    >HDFS-HA高可用集群配置</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/02/HDFS-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE/" class="article-date">
  <time datetime="2019-01-02T04:50:00.000Z" itemprop="datePublished">2019-01-02</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-HDFS-HA集群配置"><a href="#一-HDFS-HA集群配置" class="headerlink" title="一. HDFS-HA集群配置"></a>一. HDFS-HA集群配置</h3><h4 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1 环境准备"></a>1 环境准备</h4><p>(1)修改IP<br>(2)修改主机名及主机名和IP地址的映射<br>(3)关闭防火墙<br>(4)ssh免密登录<br>(5)安装JDK，配置环境变量等<br>(6)hadoop1,hadoop2,hadoop3集群已经配置好zookeeper集群</p>
<h4 id="2-规划集群"><a href="#2-规划集群" class="headerlink" title="2 规划集群"></a>2 规划集群</h4><table>
<thead>
<tr>
<th align="center">hadoop1</th>
<th align="center">hadoop2</th>
<th align="center">hadoop3</th>
</tr>
</thead>
<tbody><tr>
<td align="center">NameNode</td>
<td align="center">NameNode</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">JournalNode</td>
<td align="center">JournalNode</td>
<td align="center">JournalNode</td>
</tr>
<tr>
<td align="center">DataNode</td>
<td align="center">DataNode</td>
<td align="center">DataNode</td>
</tr>
<tr>
<td align="center">ZK</td>
<td align="center">ZK</td>
<td align="center">ZK</td>
</tr>
<tr>
<td align="center">zkfc</td>
<td align="center">zkfc</td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center">ResourceManager</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">NodeManager</td>
<td align="center">NodeManager</td>
<td align="center">NodeManager</td>
</tr>
</tbody></table>
<h4 id="3-具体步骤"><a href="#3-具体步骤" class="headerlink" title="3.具体步骤"></a>3.具体步骤</h4><p>(1).在opt目录下创建一个HA文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line"></span><br><span class="line">mkdir HA</span><br></pre></td></tr></table></figure>
<p>(2).将hadoop-2.8.4压缩包解压到/opt/HA目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.8.4.tar.gz -C /opt/HA/</span><br></pre></td></tr></table></figure>
<p>(3).配置hadoop-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>
<p>(4).配置core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/HA/hadoop-2.8.4/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置zookeeper--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:2181,hadoop2:2181,hadoop3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>(5).配置hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 完全分布式集群名称 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">              <span class="comment">&lt;!--这里主要是看自己是什么用户，我是root用户--&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 声明journalnode服务器存储目录--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/HA/hadoop-2.8.4/data/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 关闭权限检查--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>(6).配置slaves文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/HA/hadoop-2.8.4/etc/hadoop</span><br><span class="line"></span><br><span class="line">vi slaves</span><br></pre></td></tr></table></figure>
<p>修改内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure>

<p>(7).拷贝配置好的hadoop环境到其他节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/HA/* root@hadoop2:/opt/HA/</span><br><span class="line"></span><br><span class="line">scp -r /opt/HA/* root@hadoop3:/opt/HA/</span><br></pre></td></tr></table></figure>



<h4 id="4-初始化namenode"><a href="#4-初始化namenode" class="headerlink" title="4 初始化namenode"></a>4 初始化namenode</h4><p>(1).在各个JournalNode节点上，输入以下命令启动journalnode服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<p>(2).在[nn1]上，对其进行格式化，并启动：<br>hadoop1:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<p>(3).在[nn2]上，同步nn1的元数据信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<p>(4).启动[nn2]：<br>hadoop2:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTAxNjEyODllMGMxZDVmOTgucG5n?x-oss-process=image/format,png" alt="image.png"></p>
<p>(5).查看web页面显示</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTFlYjg3YjJkZTcyNDRlNmMucG5n?x-oss-process=image/format,png" alt="hadoop1"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LWJlNjNhOWE3NTg1ZWFlYWEucG5n?x-oss-process=image/format,png" alt="hadoop2"></p>
<h5 id="5-高可用HDFS集群启动"><a href="#5-高可用HDFS集群启动" class="headerlink" title="5.高可用HDFS集群启动"></a>5.高可用HDFS集群启动</h5><p>(1)关闭所有HDFS服务：<br>hadoop1</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
<p>hadoop2:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh stop namenode  </span><br></pre></td></tr></table></figure>

<p>(2)启动Zookeeper集群,每一台都启动zookeeper：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure>
<p>(3)初始化HA在Zookeeper中状态：<br>在hadoop1中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTNmMzlhYTRlNTBiN2NlNTIucG5n?x-oss-process=image/format,png"></p>
<p>(4)启动HDFS服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTMzNzVkYzIwNzI0N2UxMTMucG5n?x-oss-process=image/format,png"></p>
<p>(5)在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemin.sh start zkfc</span><br></pre></td></tr></table></figure>
<p>(若是直接设置自动切换，DFSZK Failover Controller没有启动，则需要走这一步)</p>
<h5 id="6-验证"><a href="#6-验证" class="headerlink" title="6.验证"></a>6.验证</h5><p>(1)将Active NameNode进程kill</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -9 namenode的进程id</span><br></pre></td></tr></table></figure>
<p>(2)将Active NameNode机器断开网络</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network stop</span><br></pre></td></tr></table></figure> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/" rel="tag">安装部署</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-HDFS之HA高可用概述"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/02/HDFS%E4%B9%8BHA%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A6%82%E8%BF%B0/"
    >HDFS之HA高可用概述</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/02/HDFS%E4%B9%8BHA%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A6%82%E8%BF%B0/" class="article-date">
  <time datetime="2019-01-02T04:49:00.000Z" itemprop="datePublished">2019-01-02</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-HA概述"><a href="#一-HA概述" class="headerlink" title="一.HA概述"></a>一.HA概述</h3><p>1.所谓HA(2high available)，即高可用(7*24小时不中断服务)</p>
<p>2.实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。</p>
<p>3.Hadoop2.0之前，在HDFS集群中NameNode存在单点故障(SPOF)</p>
<p>4.NameNode主要在以下两个方面影响HDFS集群<br>&nbsp;&nbsp;&nbsp;&nbsp;NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启<br>&nbsp;&nbsp;&nbsp;&nbsp;NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用<br>&nbsp;&nbsp;&nbsp;&nbsp;HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。</p>
<h3 id="二-HDFS-HA工作机制"><a href="#二-HDFS-HA工作机制" class="headerlink" title="二. HDFS-HA工作机制"></a>二. HDFS-HA工作机制</h3><p><strong>通过双namenode消除单点故障</strong></p>
<h4 id="1-HDFS-HA工作要点"><a href="#1-HDFS-HA工作要点" class="headerlink" title="1.HDFS-HA工作要点"></a>1.HDFS-HA工作要点</h4><h5 id="1-元数据管理方式需要改变："><a href="#1-元数据管理方式需要改变：" class="headerlink" title="(1).元数据管理方式需要改变："></a>(1).元数据管理方式需要改变：</h5><ul>
<li>内存中各自保存一份元数据；</li>
<li>Edits日志只有Active状态的namenode节点可以做写操作；</li>
<li>两个namenode都可以读取edits；</li>
<li>共享的edits放在一个共享存储中管理(qjournal和NFS两个主流实现)</li>
</ul>
<h5 id="2-需要一个状态管理功能模块"><a href="#2-需要一个状态管理功能模块" class="headerlink" title="(2).需要一个状态管理功能模块"></a>(2).需要一个状态管理功能模块</h5><p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</p>
<h5 id="3-必须保证两个NameNode之间能够ssh无密码登录。"><a href="#3-必须保证两个NameNode之间能够ssh无密码登录。" class="headerlink" title="(3).必须保证两个NameNode之间能够ssh无密码登录。"></a>(3).必须保证两个NameNode之间能够ssh无密码登录。</h5><h5 id="4-隔离-Fence-，即同一时刻仅仅有一个NameNode对外提供服务"><a href="#4-隔离-Fence-，即同一时刻仅仅有一个NameNode对外提供服务" class="headerlink" title="(4).隔离(Fence)，即同一时刻仅仅有一个NameNode对外提供服务"></a>(4).隔离(Fence)，即同一时刻仅仅有一个NameNode对外提供服务</h5><h4 id="2-HDFS-HA自动故障转移工作机制"><a href="#2-HDFS-HA自动故障转移工作机制" class="headerlink" title="2.HDFS-HA自动故障转移工作机制"></a>2.HDFS-HA自动故障转移工作机制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;可以使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。</p>
<p>(1)ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：</p>
<ul>
<li>(a)故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</li>
<li>(b)现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</li>
</ul>
<p>(2)ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：</p>
<ul>
<li>(a)健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
<li>(b)ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
<li>(c )基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTU0NjJiZWZiZTcwOGY2M2MucG5n?x-oss-process=image/format,png" alt="HDFS-HA故障转移机制"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce开发总结"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93/"
    >MapReduce开发总结</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93/" class="article-date">
  <time datetime="2019-01-01T09:59:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>在编写mapreduce程序时，需要考虑的几个方面：</p>
<h4 id="1-输入数据接口：InputFormat"><a href="#1-输入数据接口：InputFormat" class="headerlink" title="1.输入数据接口：InputFormat"></a>1.输入数据接口：InputFormat</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;默认使用的实现类是：TextInputFormat<br>&nbsp;&nbsp;&nbsp;&nbsp;TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。<br>&nbsp;&nbsp;&nbsp;&nbsp;KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\t）。<br>&nbsp;&nbsp;&nbsp;&nbsp;NlineInputFormat按照指定的行数N来划分切片。<br>&nbsp;&nbsp;&nbsp;&nbsp;CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。<br>&nbsp;&nbsp;&nbsp;&nbsp;用户还可以自定义InputFormat。</p>
<h4 id="2-逻辑处理接口：Mapper"><a href="#2-逻辑处理接口：Mapper" class="headerlink" title="2.逻辑处理接口：Mapper"></a>2.逻辑处理接口：Mapper</h4><p>   用户根据业务需求实现其中三个方法：map()   setup()   cleanup () </p>
<h4 id="3-Partitioner分区"><a href="#3-Partitioner分区" class="headerlink" title="3.Partitioner分区"></a>3.Partitioner分区</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces<br>    如果业务上有特别的需求，可以自定义分区。</p>
<h4 id="4-Comparable排序"><a href="#4-Comparable排序" class="headerlink" title="4.Comparable排序"></a>4.Comparable排序</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。</p>
<ul>
<li>部分排序：对最终输出的每一个文件进行内部排序。</li>
<li>全排序：对所有数据进行排序，通常只有一个Reduce。</li>
<li>二次排序：排序的条件有两个。</li>
</ul>
<h4 id="5-Combiner合并"><a href="#5-Combiner合并" class="headerlink" title="5.Combiner合并"></a>5.Combiner合并</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Combiner合并可以提高程序执行效率，减少io传输。但是使用时必须不能影响原有的业务处理结果。</p>
<h4 id="6-reduce端分组：Groupingcomparator"><a href="#6-reduce端分组：Groupingcomparator" class="headerlink" title="6.reduce端分组：Groupingcomparator"></a>6.reduce端分组：Groupingcomparator</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑。<br>&nbsp;&nbsp;&nbsp;&nbsp;自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果。然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）。这样，我们要取的最大值就是reduce()方法中传进来key。</p>
<h4 id="7-逻辑处理接口：Reducer"><a href="#7-逻辑处理接口：Reducer" class="headerlink" title="7.逻辑处理接口：Reducer"></a>7.逻辑处理接口：Reducer</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () </p>
<h4 id="8-输出数据接口：OutputFormat"><a href="#8-输出数据接口：OutputFormat" class="headerlink" title="8.输出数据接口：OutputFormat"></a>8.输出数据接口：OutputFormat</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对向目标文本文件中输出为一行。<br>&nbsp;&nbsp;&nbsp;&nbsp;SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。<br>&nbsp;&nbsp;&nbsp;&nbsp;用户还可以自定义OutputFormat。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapRedude之Join，数据清洗，计数器应用"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapRedude%E4%B9%8BJoin%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%8C%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8/"
    >MapRedude之Join，数据清洗，计数器应用</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapRedude%E4%B9%8BJoin%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%8C%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8/" class="article-date">
  <time datetime="2019-01-01T09:57:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="一-Join多种应用"><a href="#一-Join多种应用" class="headerlink" title="一. Join多种应用"></a>一. Join多种应用</h3><h4 id="1-Reduce-join"><a href="#1-Reduce-join" class="headerlink" title="1.Reduce join"></a>1.Reduce join</h4><h5 id="1-原理："><a href="#1-原理：" class="headerlink" title="(1).原理："></a>(1).原理：</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。</p>
<h5 id="2-该方法的缺点"><a href="#2-该方法的缺点" class="headerlink" title="(2).该方法的缺点"></a>(2).该方法的缺点</h5><p>这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。</p>
<h5 id="3-案例实操"><a href="#3-案例实操" class="headerlink" title="(3).案例实操"></a>(3).案例实操</h5><h4 id="2-Map-join（Distributedcache分布式缓存）"><a href="#2-Map-join（Distributedcache分布式缓存）" class="headerlink" title="2.Map join（Distributedcache分布式缓存）"></a>2.Map join（Distributedcache分布式缓存）</h4><h5 id="1-使用场景：一张表十分小、一张表很大。"><a href="#1-使用场景：一张表十分小、一张表很大。" class="headerlink" title="(1).使用场景：一张表十分小、一张表很大。"></a>(1).使用场景：一张表十分小、一张表很大。</h5><h5 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="(2).解决方案"></a>(2).解决方案</h5><p>在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。</p>
<h6 id="3-具体办法：采用distributedcache"><a href="#3-具体办法：采用distributedcache" class="headerlink" title="(3).具体办法：采用distributedcache"></a>(3).具体办法：采用distributedcache</h6><p>(a)在mapper的setup阶段，将文件读取到缓存集合中。<br>(b)在驱动函数中加载缓存。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:/e:/mapjoincache/pd.txt&quot;</span>));<span class="comment">// 缓存普通文件到task运行节点</span></span><br></pre></td></tr></table></figure>
<h5 id="4-实操案例："><a href="#4-实操案例：" class="headerlink" title="(4).实操案例："></a>(4).实操案例：</h5><h3 id="二-数据清洗-ETL"><a href="#二-数据清洗-ETL" class="headerlink" title="二. 数据清洗(ETL)"></a>二. 数据清洗(ETL)</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在运行核心业务Mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序。</p>
<h4 id="2-实操案例"><a href="#2-实操案例" class="headerlink" title="2.实操案例"></a>2.实操案例</h4><h3 id="三-计数器应用"><a href="#三-计数器应用" class="headerlink" title="三.计数器应用"></a>三.计数器应用</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p>
<h4 id="1-API"><a href="#1-API" class="headerlink" title="1.API"></a>1.API</h4><p>(1)采用枚举的方式统计计数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">MyCounter</span></span>&#123;MALFORORMED,NORMAL&#125;</span><br><span class="line"><span class="comment">//对枚举定义的自定义计数器加1</span></span><br><span class="line">context.getCounter(MyCounter.MALFORORMED).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>(2)采用计数器组、计数器名称的方式统计</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">context.getCounter(<span class="string">&quot;counterGroup&quot;</span>, <span class="string">&quot;countera&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//组名和计数器名称随便起，但最好有意义。</span></span><br></pre></td></tr></table></figure>
<p>(3)计数结果在程序运行后的控制台上查看。</p>
<h4 id="2-案例实操"><a href="#2-案例实操" class="headerlink" title="2.案例实操"></a>2.案例实操</h4> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之OutputFormat数据输出"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E4%B9%8BOutputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/"
    >MapReduce之OutputFormat数据输出</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E4%B9%8BOutputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/" class="article-date">
  <time datetime="2019-01-01T09:56:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-OutputFormat接口实现类"><a href="#1-OutputFormat接口实现类" class="headerlink" title="1.OutputFormat接口实现类"></a>1.OutputFormat接口实现类</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p>
<h5 id="1-文本输出TextOutputFormat"><a href="#1-文本输出TextOutputFormat" class="headerlink" title="(1).文本输出TextOutputFormat"></a>(1).文本输出TextOutputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。</p>
<h5 id="2-SequenceFileOutputFormat"><a href="#2-SequenceFileOutputFormat" class="headerlink" title="(2).SequenceFileOutputFormat"></a>(2).SequenceFileOutputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p>
<h5 id="3-自定义OutputFormat"><a href="#3-自定义OutputFormat" class="headerlink" title="(3).自定义OutputFormat"></a>(3).自定义OutputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;根据用户需求，自定义实现输出。</p>
<h4 id="2-自定义OutputFormat"><a href="#2-自定义OutputFormat" class="headerlink" title="2.自定义OutputFormat"></a>2.自定义OutputFormat</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;为了实现控制最终文件的输出路径，可以自定义OutputFormat。<br>&nbsp;&nbsp;&nbsp;&nbsp;要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现。</p>
<h5 id="1-自定义OutputFormat步骤"><a href="#1-自定义OutputFormat步骤" class="headerlink" title="(1).自定义OutputFormat步骤"></a>(1).自定义OutputFormat步骤</h5><p>(a)自定义一个类继承FileOutputFormat。<br>(b)改写recordwriter，具体改写输出数据的方法write()。</p>
<h5 id="2-实操案例："><a href="#2-实操案例：" class="headerlink" title="(2).实操案例："></a>(2).实操案例：</h5> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之ReduceTask工作机制"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E4%B9%8BReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/"
    >MapReduce之ReduceTask工作机制</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E4%B9%8BReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/" class="article-date">
  <time datetime="2019-01-01T09:55:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-设置ReduceTask并行度（个数）"><a href="#1-设置ReduceTask并行度（个数）" class="headerlink" title="1.设置ReduceTask并行度（个数）"></a>1.设置ReduceTask并行度（个数）</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>
<h4 id="2-注意"><a href="#2-注意" class="headerlink" title="2.注意"></a>2.注意</h4><p>(1)reducetask=0 ，表示没有reduce阶段，输出文件个数和map个数一致。</p>
<p>(2)reducetask默认值就是1，所以输出文件个数为一个。</p>
<p>(3)如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜</p>
<p>(4)reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。</p>
<p>(5)具体多少个reducetask，需要根据集群性能而定。</p>
<p>(6)如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。</p>
<h4 id="3-实验：测试reducetask多少合适。"><a href="#3-实验：测试reducetask多少合适。" class="headerlink" title="3.实验：测试reducetask多少合适。"></a>3.实验：测试reducetask多少合适。</h4><p>(1)实验环境：1个master节点，16个slave节点：CPU:8GHZ，内存: 2G</p>
<p>(2)实验结论：</p>
<p>表1 改变reduce task （数据量为1GB）</p>
<table>
<thead>
<tr>
<th align="center">maptask=16</th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">Reduce task</td>
<td align="center">1</td>
<td align="center">5</td>
<td align="center">10</td>
<td align="center">15</td>
<td align="center">16</td>
<td align="center">20</td>
<td align="center">25</td>
<td align="center">30</td>
<td align="center">45</td>
<td align="center">60</td>
</tr>
<tr>
<td align="center">总时间</td>
<td align="center">892</td>
<td align="center">146</td>
<td align="center">110</td>
<td align="center">92</td>
<td align="center">88</td>
<td align="center">100</td>
<td align="center">128</td>
<td align="center">101</td>
<td align="center">145</td>
<td align="center">104</td>
</tr>
</tbody></table>
<h4 id="4-ReduceTask工作机制"><a href="#4-ReduceTask工作机制" class="headerlink" title="4.ReduceTask工作机制"></a>4.ReduceTask工作机制</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTliYzNmZjE5MmJmYjczYTEucG5n?x-oss-process=image/format,png" alt="ReduceTask工作机制"></p>
<p>(1)Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>(2)Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>(3)Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>
<p>(4)Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Shuffle机制"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6/"
    >MapReduce之Shuffle机制</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6/" class="article-date">
  <time datetime="2019-01-01T09:54:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-Shuffle机制"><a href="#1-Shuffle机制" class="headerlink" title="1. Shuffle机制"></a>1. Shuffle机制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程(即将map输出作为输入传给reducer)称为shuffle。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTcwZTMyM2E2NDg4YWYzOGYucG5n?x-oss-process=image/format,png"><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTRlZDQwNzk0NmFlNDNiZGEucG5n?x-oss-process=image/format,png" alt="shuffle机制"></p>
<h4 id="2-Partition分区"><a href="#2-Partition分区" class="headerlink" title="2.Partition分区"></a>2.Partition分区</h4><h5 id="0-问题引出："><a href="#0-问题引出：" class="headerlink" title="(0).问题引出："></a>(0).问题引出：</h5><p>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<h5 id="1-默认partition分区"><a href="#1-默认partition分区" class="headerlink" title="(1).默认partition分区"></a>(1).默认partition分区</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p>
<h5 id="2-自定义Partitioner步骤"><a href="#2-自定义Partitioner步骤" class="headerlink" title="(2).自定义Partitioner步骤"></a>(2).自定义Partitioner步骤</h5><p>(a)自定义类继承Partitioner，重写getPartition()方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">       <span class="meta">@Override</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">              <span class="comment">// 1 获取电话号码的前三位</span></span><br><span class="line">              String preNum = key.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">              partition = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">              <span class="comment">// 2 判断是哪个省</span></span><br><span class="line">              <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">                     partition = <span class="number">0</span>;</span><br><span class="line">              &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">                     partition = <span class="number">1</span>;</span><br><span class="line">              &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">                     partition = <span class="number">2</span>;</span><br><span class="line">              &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">                     partition = <span class="number">3</span>;</span><br><span class="line">              &#125;</span><br><span class="line">              <span class="keyword">return</span> partition;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>(b)在job驱动中，设置自定义partitioner：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure>

<p>(c )自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>
<h5 id="3-注意："><a href="#3-注意：" class="headerlink" title="(3).注意："></a>(3).注意：</h5><ul>
<li><p>如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</p>
</li>
<li><p>如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</p>
</li>
<li><p>如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000；</p>
</li>
</ul>
<p>例如：假设自定义分区数为5，则<br>(a)job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件<br>(b)job.setNumReduceTasks(2);会报错<br>(c )job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件</p>
<h6 id="4-案例实操"><a href="#4-案例实操" class="headerlink" title="(4).案例实操"></a>(4).案例实操</h6><h4 id="3-WritableComparable排序"><a href="#3-WritableComparable排序" class="headerlink" title="3.WritableComparable排序"></a>3.WritableComparable排序</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。</p>
<p>每个阶段的默认排序</p>
<h5 id="1-排序的分类："><a href="#1-排序的分类：" class="headerlink" title="(1).排序的分类："></a>(1).排序的分类：</h5><h6 id="a-部分排序："><a href="#a-部分排序：" class="headerlink" title="(a)部分排序："></a>(a)部分排序：</h6><p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。</p>
<h6 id="b-全排序："><a href="#b-全排序：" class="headerlink" title="(b)全排序："></a>(b)全排序：</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。</p>
<h6 id="c-辅助排序：-GroupingComparator分组"><a href="#c-辅助排序：-GroupingComparator分组" class="headerlink" title="(c )辅助排序：(GroupingComparator分组)"></a>(c )辅助排序：(GroupingComparator分组)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。</p>
<h6 id="d-二次排序："><a href="#d-二次排序：" class="headerlink" title="(d)二次排序："></a>(d)二次排序：</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p>
<h5 id="2-自定义排序WritableComparable"><a href="#2-自定义排序WritableComparable" class="headerlink" title="(2).自定义排序WritableComparable"></a>(2).自定义排序WritableComparable</h5><h6 id="a-原理分析"><a href="#a-原理分析" class="headerlink" title="(a)原理分析"></a>(a)原理分析</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="b-案例实操"><a href="#b-案例实操" class="headerlink" title="(b)案例实操"></a>(b)案例实操</h6><h4 id="4-GroupingComparator分组（辅助排序）"><a href="#4-GroupingComparator分组（辅助排序）" class="headerlink" title="4.GroupingComparator分组（辅助排序）"></a>4.GroupingComparator分组（辅助排序）</h4><p>(1).对reduce阶段的数据根据某一个或几个字段进行分组。</p>
<p>(2).案例实操</p>
<h4 id="5-Combiner合并"><a href="#5-Combiner合并" class="headerlink" title="5.Combiner合并"></a>5.Combiner合并</h4><h5 id="0-在分布式的架构中，分布式文件系统HDFS，和分布式运算程序编程框架mapreduce。"><a href="#0-在分布式的架构中，分布式文件系统HDFS，和分布式运算程序编程框架mapreduce。" class="headerlink" title="(0).在分布式的架构中，分布式文件系统HDFS，和分布式运算程序编程框架mapreduce。"></a>(0).在分布式的架构中，分布式文件系统HDFS，和分布式运算程序编程框架mapreduce。</h5><ul>
<li>HDFS:不怕大文件，怕很多小文件</li>
<li>mapreduce :怕数据倾斜</li>
</ul>
<p>那么mapreduce是如果解决多个小文件的问题呢？</p>
<h5 id="1-mapreduce关于大量小文件的优化策略"><a href="#1-mapreduce关于大量小文件的优化策略" class="headerlink" title="(1).mapreduce关于大量小文件的优化策略"></a>(1).mapreduce关于大量小文件的优化策略</h5><p>(a)默认情况下，<strong>TextInputFormat</strong>对任务的切片机制是按照<strong>文件</strong>规划切片，不管有多少个小文件，都会是单独的切片，都<strong>会交给一个maptask</strong>，这样，如果有大量的小文件<br>就会产生大量的maptask，处理效率极端底下</p>
<p>(b)优化策略</p>
<ul>
<li>最好的方法：在数据处理的最前端（预处理、采集），就将小文件合并成大文件，在上传到HDFS做后续的分析</li>
<li>补救措施：如果已经是大量的小文件在HDFS中了，可以使用另一种inputformat来做切片（CombineFileInputformat），它的切片逻辑跟<strong>TextInputformat</strong>不同,它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask了<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果不设置InputFormat，它默认的用的是TextInputFormat.class</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*CombineTextInputFormat为系统自带的组件类</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> * setMinInputSplitSize 中的2048是表示n个小文件之和不能大于2048</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> * setMaxInputSplitSize 中的4096是     当满足setMinInputSplitSize中的2048情况下  在满足n+1个小文件之和不能大于4096</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line">CombineTextInputFormat.setMinInputSplitSize(job, <span class="number">2048</span>);</span><br><span class="line"></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4096</span>);</span><br></pre></td></tr></table></figure>
<h5 id="2-示例"><a href="#2-示例" class="headerlink" title="(2).示例"></a>(2).示例</h5><h6 id="a-输入数据：准备5个小文件"><a href="#a-输入数据：准备5个小文件" class="headerlink" title="(a)输入数据：准备5个小文件"></a>(a)输入数据：准备5个小文件</h6><h6 id="b-实现过程"><a href="#b-实现过程" class="headerlink" title="(b)实现过程"></a>(b)实现过程</h6></li>
<li>(a)不做任何处理，运行需求1中的wordcount程序，观察切片个数为5</li>
<li>(b)在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line"></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.**<span class="class"><span class="keyword">class</span>**)</span>;</span><br><span class="line"></span><br><span class="line">CombineTextInputFormat.*setMaxInputSplitSize*(job, <span class="number">4</span>*<span class="number">1024</span>*<span class="number">1024</span>);<span class="comment">// 4m</span></span><br><span class="line"></span><br><span class="line">CombineTextInputFormat.*setMinInputSplitSize*(job, <span class="number">2</span>*<span class="number">1024</span>*<span class="number">1024</span>);<span class="comment">// 2m</span></span><br></pre></td></tr></table></figure>
注：在看number of splits时，和最大值(MaxSplitSize)有关、总体规律就是和低于最大值是一片、高于最大值1.5倍+，则为两片；高于最大值2倍以上则向下取整，比如文件大小65MB，切片最大值为4MB,那么切片为16个.总体来说，切片差值不超过1个，不影响整体性能</li>
</ul>
<h5 id="3-自定义Combiner实现步骤："><a href="#3-自定义Combiner实现步骤：" class="headerlink" title="(3).自定义Combiner实现步骤："></a>(3).自定义Combiner实现步骤：</h5><p>(a)自定义一个combiner继承Reducer，重写reduce方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">       <span class="meta">@Override</span></span><br><span class="line">       <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总操作</span></span><br><span class="line">              <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">              <span class="keyword">for</span>(IntWritable v :values)&#123;</span><br><span class="line">                     count = v.get();</span><br><span class="line">              &#125;</span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">              context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(b)在job驱动类中设置：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之MapTask工作机制"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E4%B9%8BMapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/"
    >MapReduce之MapTask工作机制</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E4%B9%8BMapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/" class="article-date">
  <time datetime="2019-01-01T09:53:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-并行度决定机制"><a href="#1-并行度决定机制" class="headerlink" title="1.并行度决定机制"></a>1.并行度决定机制</h4><h5 id="1-问题引出"><a href="#1-问题引出" class="headerlink" title="(1).问题引出"></a>(1).问题引出</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？</p>
<h5 id="2-MapTask并行度决定机制"><a href="#2-MapTask并行度决定机制" class="headerlink" title="(2).MapTask并行度决定机制"></a>(2).MapTask并行度决定机制</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTkyOTQ2ODVmZjkyMTc5MGYucG5n?x-oss-process=image/format,png" alt="数据切片及Maptask并行度决定机制"></p>
<h4 id="2-MapTask工作机制"><a href="#2-MapTask工作机制" class="headerlink" title="2.MapTask工作机制"></a>2.MapTask工作机制</h4><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80MzkxNDA3LTgzNjIxZjZhOWZiMjc0ZjAucG5n?x-oss-process=image/format,png" alt="MapTask工作机制"></p>
<p>(1)Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p>
<p>(2)Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p>
<p>(3)Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>(4)Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;溢写阶段详情：</p>
<ul>
<li><p>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
</li>
<li><p>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
</li>
<li><p>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
</li>
</ul>
<p>(5)Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。<br>&nbsp;&nbsp;&nbsp;&nbsp;在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。<br>&nbsp;&nbsp;&nbsp;&nbsp;让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之InputFormat数据输入"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/MapReduce%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/"
    >MapReduce之InputFormat数据输入</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/MapReduce%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/" class="article-date">
  <time datetime="2019-01-01T09:51:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-Job提交流程和切片源码详解"><a href="#1-Job提交流程和切片源码详解" class="headerlink" title="1.Job提交流程和切片源码详解"></a>1.Job提交流程和切片源码详解</h4><h5 id="1-job提交流程源码详解"><a href="#1-job提交流程源码详解" class="headerlink" title="(1).job提交流程源码详解"></a>(1).job提交流程源码详解</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line">     </span><br><span class="line">    submit();        </span><br><span class="line">	connect();	// 1建立连接</span><br><span class="line">		</span><br><span class="line">	new Cluster(getConfiguration());  // 1）创建提交job的代理</span><br><span class="line">			</span><br><span class="line">	initialize(jobTrackAddr, conf); // （1）判断是本地yarn还是远程</span><br><span class="line">	</span><br><span class="line">    submitter.submitJobInternal(Job.this, cluster)       // 2 提交job</span><br><span class="line">	</span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);  // 1）创建给集群提交数据的Stag路径</span><br><span class="line">	</span><br><span class="line">	JobID jobId = submitClient.getNewJobID();    // 2）获取jobid ，并创建job路径</span><br><span class="line"></span><br><span class="line">    copyAndConfigureFiles(job, submitJobDir);	// 3）拷贝jar包到集群</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">    writeSplits(job, submitJobDir);// 4）计算切片，生成切片规划文件</span><br><span class="line">	maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">	input.getSplits(job);</span><br><span class="line">                          // 5）向Stag路径写xml配置文件</span><br><span class="line">    writeConf(conf, submitJobFile);</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line">                        // 6）提交job,返回提交状态</span><br><span class="line">    status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>
<h5 id="2-FileInputFormat源码解析-input-getSplits-job-这里留一个坑"><a href="#2-FileInputFormat源码解析-input-getSplits-job-这里留一个坑" class="headerlink" title="(2).FileInputFormat源码解析(input.getSplits(job))(这里留一个坑)"></a>(2).FileInputFormat源码解析(input.getSplits(job))(<strong>这里留一个坑</strong>)</h5><p>(a)找到你数据存储的目录。<br>(b)开始遍历处理（规划切片）目录下的每一个文件<br>(c )遍历第一个文件ss.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(a)获取文件大小fs.sizeOf(ss.txt);</span><br><span class="line">(b)计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M</span><br><span class="line">(c)默认情况下，切片大小=blocksize</span><br><span class="line">(d)开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）</span><br><span class="line">(e)将切片信息写到一个切片规划文件中</span><br><span class="line">(f)整个切片的核心过程在getSplit()方法中完成。</span><br><span class="line">(g)数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</span><br><span class="line">(h)注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。</span><br></pre></td></tr></table></figure>
<p>(d)提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。</p>
<h4 id="2-FileInputFormat切片机制"><a href="#2-FileInputFormat切片机制" class="headerlink" title="2.FileInputFormat切片机制"></a>2.FileInputFormat切片机制</h4><h5 id="1-FileInputFormat中默认的切片机制："><a href="#1-FileInputFormat中默认的切片机制：" class="headerlink" title="(1).FileInputFormat中默认的切片机制："></a>(1).FileInputFormat中默认的切片机制：</h5><p>(a)简单地按照文件的内容长度进行切片</p>
<p>(b)切片大小，默认等于block大小</p>
<p>(c)切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p>
<p>比如待处理数据有两个文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file1.txt    320M</span><br><span class="line"></span><br><span class="line">file2.txt    10M</span><br></pre></td></tr></table></figure>
<p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">file1.txt.split1--  0~128</span><br><span class="line"></span><br><span class="line">file1.txt.split2--  128~256</span><br><span class="line"></span><br><span class="line">file1.txt.split3--  256~320</span><br><span class="line"></span><br><span class="line">file2.txt.split1--  0~10M</span><br></pre></td></tr></table></figure>

<h5 id="2-FileInputFormat切片大小的参数配置"><a href="#2-FileInputFormat切片大小的参数配置" class="headerlink" title="(2).FileInputFormat切片大小的参数配置"></a>(2).FileInputFormat切片大小的参数配置</h5><p>(a)通过分析源码，在FileInputFormat中，计算切片大小的逻辑:Math.max(minSize, Math.min(maxSize, blockSize)); </p>
<p>(b)切片主要由这几个值来运算决定</p>
<ul>
<li><p>mapreduce.input.fileinputformat.split.minsize=1 默认值为1</p>
</li>
<li><p>mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue,因此，默认情况下，切片大小=blocksize。</p>
</li>
<li><p>maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。</p>
</li>
<li><p>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。</p>
</li>
</ul>
<h5 id="3-获取切片信息API"><a href="#3-获取切片信息API" class="headerlink" title="(3).获取切片信息API"></a>(3).获取切片信息API</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br></pre></td></tr></table></figure>

<h4 id="3-CombineTextInputFormat切片机制"><a href="#3-CombineTextInputFormat切片机制" class="headerlink" title="3.CombineTextInputFormat切片机制"></a>3.CombineTextInputFormat切片机制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;关于大量小文件的优化策略</p>
<h5 id="1-默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。"><a href="#1-默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。" class="headerlink" title="(1).默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。"></a>(1).默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。</h5><h5 id="2-优化策略"><a href="#2-优化策略" class="headerlink" title="(2).优化策略"></a>(2).优化策略</h5><ul>
<li><p>(a)最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到HDFS做后续分析。</p>
</li>
<li><p>(b)补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。</p>
</li>
<li><p>(c)优先满足最小切片大小，不超过最大切片大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</span><br><span class="line">CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m</span><br><span class="line">举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-具体实现步骤"><a href="#3-具体实现步骤" class="headerlink" title="(3).具体实现步骤"></a>(3).具体实现步骤</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  如果不设置InputFormat,它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class)</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);<span class="comment">// 4m</span></span><br><span class="line">CombineTextInputFormat.setMinInputSplitSize(job, <span class="number">2097152</span>);<span class="comment">// 2m</span></span><br></pre></td></tr></table></figure>
<h5 id="4-案例实操"><a href="#4-案例实操" class="headerlink" title="(4).案例实操"></a>(4).案例实操</h5><h4 id="4-InputFormat接口实现类"><a href="#4-InputFormat接口实现类" class="headerlink" title="4.InputFormat接口实现类"></a>4.InputFormat接口实现类</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;MapReduce任务的输入文件一般是存储在HDFS里面。输入的文件格式包括：基于行的日志文件、二进制格式文件等。这些文件一般会很大，达到数十GB，甚至更大。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;InputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。</p>
<h5 id="1-TextInputFormat"><a href="#1-TextInputFormat" class="headerlink" title="(1).TextInputFormat"></a>(1).TextInputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;TextInputFormat是默认的InputFormat。每条记录是一行输入。键是LongWritable类型，存储该行在整个文件中的字节偏移量。值是这行的内容，不包括任何行终止符（换行符和回车符）<br>&nbsp;&nbsp;&nbsp;&nbsp;以下是一个示例，比如，一个分片包含了如下4条文本记录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;每条记录表示为以下键/值对：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>
<p>很明显，键并不是行号。一般情况下，很难取得行号，因为文件按字节而不是按行切分为分片。</p>
<h5 id="2-KeyValueTextInputFormat"><a href="#2-KeyValueTextInputFormat" class="headerlink" title="(2).KeyValueTextInputFormat"></a>(2).KeyValueTextInputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “ “);来设定分隔符。默认分隔符是tab（\t）。<br>&nbsp;&nbsp;&nbsp;&nbsp;以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">line1 ——&gt;Rich learning form</span><br><span class="line">line2 ——&gt;Intelligent learning engine</span><br><span class="line">line3 ——&gt;Learning more convenient</span><br><span class="line">line4 ——&gt;From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;每条记录表示为以下键/值对：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(line1,Rich learning form)</span><br><span class="line">(line2,Intelligent learning engine)</span><br><span class="line">(line3,Learning more convenient)</span><br><span class="line">(line4,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;此时的键是每行排在制表符之前的Text序列。</p>
<h5 id="3-NLineInputFormat"><a href="#3-NLineInputFormat" class="headerlink" title="(3).NLineInputFormat"></a>(3).NLineInputFormat</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数(20)，如果不整除，切片数=商+1。<br>&nbsp;&nbsp;&nbsp;&nbsp;以下是一个示例，仍然以上面的4行输入为例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;例如，如果N是2，则每个输入分片包含两行。开启2个maptask。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;另一个 mapper 则收到后两行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;这里的键和值与TextInputFormat生成的一样。</p>
<h4 id="5-自定义InputFormat"><a href="#5-自定义InputFormat" class="headerlink" title="5.自定义InputFormat"></a>5.自定义InputFormat</h4><h5 id="1-概述"><a href="#1-概述" class="headerlink" title="(1).概述"></a>(1).概述</h5><p>(a)自定义一个类继承FileInputFormat</p>
<p>(b)改写RecordReader，实现一次读取一个完整文件封装为KV。</p>
<p>(c )在输出时使用SequenceFileOutPutFormat输出合并文件。</p>
<h5 id="2-案例实操"><a href="#2-案例实操" class="headerlink" title="(2).案例实操"></a>(2).案例实操</h5> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hadoop序列化"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/01/01/Hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"
    >Hadoop序列化</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2019/01/01/Hadoop%E5%BA%8F%E5%88%97%E5%8C%96/" class="article-date">
  <time datetime="2019-01-01T09:50:00.000Z" itemprop="datePublished">2019-01-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="1-为什么要序列化？"><a href="#1-为什么要序列化？" class="headerlink" title="1 为什么要序列化？"></a>1 为什么要序列化？</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</p>
<h4 id="2-什么是序列化？"><a href="#2-什么是序列化？" class="headerlink" title="2 什么是序列化？"></a>2 什么是序列化？</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。<br>反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。</p>
<h4 id="3-为什么不用Java的序列化？"><a href="#3-为什么不用Java的序列化？" class="headerlink" title="3 为什么不用Java的序列化？"></a>3 为什么不用Java的序列化？</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。</p>
<h4 id="4-为什么序列化对Hadoop很重要？"><a href="#4-为什么序列化对Hadoop很重要？" class="headerlink" title="4 为什么序列化对Hadoop很重要？"></a>4 为什么序列化对Hadoop很重要？</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;因为Hadoop在集群之间进行通讯或者RPC调用的时候，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解Hadoop的序列化机制。<br>&nbsp;&nbsp;&nbsp;&nbsp;序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点：</p>
<ul>
<li>紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资</li>
<li>快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的；</li>
<li>可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文；</li>
<li>互操作：能支持不同语言写的客户端和服务端进行交互；</li>
</ul>
<h4 id="5-常用数据序列化类型"><a href="#5-常用数据序列化类型" class="headerlink" title="5.常用数据序列化类型"></a>5.常用数据序列化类型</h4><p>常用的数据类型对应的hadoop数据序列化类型</p>
<table>
<thead>
<tr>
<th align="center">Java类型</th>
<th align="center">Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td align="center">boolean</td>
<td align="center">BooleanWritable</td>
</tr>
<tr>
<td align="center">byte</td>
<td align="center">ByteWritable</td>
</tr>
<tr>
<td align="center">int</td>
<td align="center">IntWritable</td>
</tr>
<tr>
<td align="center">float</td>
<td align="center">FloatWritable</td>
</tr>
<tr>
<td align="center">long</td>
<td align="center">LongWritable</td>
</tr>
<tr>
<td align="center">double</td>
<td align="center">DoubleWritable</td>
</tr>
<tr>
<td align="center">string</td>
<td align="center">Text</td>
</tr>
<tr>
<td align="center">map</td>
<td align="center">MapWritable</td>
</tr>
<tr>
<td align="center">array</td>
<td align="center">ArrayWritable</td>
</tr>
</tbody></table>
<h4 id="6-自定义bean对象实现序列化接口（Writable）"><a href="#6-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="6 自定义bean对象实现序列化接口（Writable）"></a>6 自定义bean对象实现序列化接口（Writable）</h4><h5 id="1-自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项"><a href="#1-自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项" class="headerlink" title="(1)自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项"></a>(1)自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项</h5><p>(a)必须实现Writable接口</p>
<p>(b)反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>(c )重写序列化方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    out.writeLong(upFlow);</span><br><span class="line"></span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line"></span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(d)重写反序列化方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    upFlow = in.readLong();</span><br><span class="line"></span><br><span class="line">    downFlow = in.readLong();</span><br><span class="line"></span><br><span class="line">    sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(e)注意反序列化的顺序和序列化的顺序完全一致</p>
<p>(f)要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</p>
<p>(g)如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/25/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="page-number" href="/page/25/">25</a><span class="page-number current">26</span><a class="page-number" href="/page/27/">27</a><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/27/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> Movle
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="https://img-blog.csdnimg.cn/20200609161448519.jpg" alt="Movle"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>